
\chapter{First principle methods}


\section{The residual method}

\subsection{Introduction}

The residual method is very simple...


\subsection{Proof of consistency: A tale of two bounds}

The setup was the linear \textbf{ANM}:

\[ \begin{cases} 
    & Y = aX + E_Y  \\
    & X \bigCI E_Y,\; X \thicksim p_x,\; E_Y \thicksim p_{E_Y}  
 \end{cases}
\]

From $n$ samples $(X_i, Y_i)$ we estimate $\hat{f}_Y$ by regressing $X$ on $Y$ and 
$\hat{f}_X$ for the reverse model. We then comput the residuals

\begin{align}
    &  \hat{e}_Y = Y - \hat{f}_Y(X)\\
    &  \hat{e}_X = X - \hat{f}_X(Y)
\end{align}

We note that for the ease of analysis, it would first be wise to use some fraction 
of the data to first estimate the regression, and then use the remanining for the test.

For $n$ large enough we have that 

$$
    \hat{e}_Y \approx E_Y \thicksim P_{E_Y}
$$

The idea is then to first discretise\footnote{We do so in a naive manner we split
it uniformly into $m$ bins.} $P_{E_Y}$ into $m$ bins, call this discrete distribution
$Q$. We apply the same discretization to obtain $B = (b_1, ..., b_m)$ from $\hat{e}_Y$
and $\tilde{B} = (\tilde{b}_1, ..., \tilde{b}_m)$ from $\hat{e}_X$.

We then decide the causal direction as follows

\[ \begin{cases} 
    & X \rightarrow Y \quad \text{if} \quad C \leq W  \\
    & Y \rightarrow X \quad \text{if} \quad C > W  
 \end{cases}
\]

Where 

$$
    C = \norm{B - U}_{L_1} 
$$
$$
    W = \norm{\tilde{B} - U}_{L_1}
$$


s.t. $U = (\frac{1}{m}, ..., \frac{1}{m})$.

Given our assumption about the \textbf{ANM}, the probability to outpout the correct causal direction is:

$$
   P_{\text{correct}} = \mathbb{P}\left[C \leq W\right] 
$$

We next upper bound thise quantity in order to show consistency

\begin{align}
    \mathbb{P}\left[C \leq W\right] &\geq \mathbb{P}\left[ \underset{\tau \in \mathbb{Q}}{\bigcup} C \leq \tau \cap W > \tau \right] \\
    &\geq \mathbb{P}\left[C \leq \tau \cap W > \tau \right] \\
    &\geq \mathbb{P}\left[C \leq \tau \right] - \mathbb{P}\left[W \leq \tau \right]
\end{align}

The first inequality is due to the fact that we are only taking the union in the rationals\footnote{We note that
we can only take unions over countable sets; recall also that the rationals are dense in the irrationals, so the
inequality is very close to equality (and in practice and among friends it would be).}. The second inequality is done by 
looking at the probability of a fixed $\tau$; and the final one follows by:

$$
    1 \geq \mathbb{P}\left[ C \leq \tau \cup W > \tau \right] = 
    \mathbb{P}\left[ C \leq \tau \right] + \mathbb{P}\left[ W > \tau \right] - \mathbb{P}\left[ C \leq \tau \cap W > \tau \right]
$$

We will next find appropriate bounds for $\mathbb{P}\left[ C \leq \tau \right]$ and $\mathbb{P}\left[W \leq \tau \right]$.

\subsection{Bounding the false false postive}

We will first lower bound $\mathbb{P}\left[ C \leq \tau \right]$ by upper bounding the complement event.

\begin{align}
    \mathbb{P}\left[ C \geq \tau \right] &= \mathbb{P}\left[ \sum_{i = 1}^{m} \abs{b_i - \frac{1}{m}} \geq \tau \right]  \\
    &\leq \mathbb{P}\left[ m \operatorname{max}_{i} \abs{b_i - \frac{1}{m}} \geq \tau \right]  \\
    &= \mathbb{P}\left[ \bigcup_{i} \; \abs{b_i - \frac{1}{m}} \geq \frac{\tau}{m} \right]  \\
    &\leq m \; \mathbb{P}\left[ \abs{b_0 - \frac{1}{m}} \geq \frac{\tau}{m} \right] \\
    &\leq m2\exp \left( -2n \frac{\tau^2}{m^2} \right)
\end{align}

The second to last inequality follows by the union bound and by noting that all $b_i$s are the same since they
are discretized empirical distribution coming from a uniform source. For the final inequality we use Hoeffding's
inequality.

\subsection{Bounding the false negatives}

Recall that what is left to bound is the following quanity, $\mathbb{P}\left[W \leq \tau \right]$; 
for this we first define the following set of probability distributions:

$$
    \Gamma_\tau = \{ \pi \in  \Delta_m : \norm{\pi - U}_{L_1} \leq \tau \}
$$

Where the $\Delta_m$ is the $m$ dimensional simple and $U$ the uniform vector as before.

Observe that: 

$$
    \{ W \leq \tau \} = \{ \tilde{B} \in \Gamma_\tau \}
$$

In essence, we are asking: "what is the chance that the realisation of $\tilde{B}$ – which is the 
empricial distribution of some distribution $Q$ – lies inside some set of distribtions $\Gamma_\tau$.

We note that bounding this kind of event is exactly what Sanov's theorem\footnote{See the section on Information Theory and statistics in 
\cite{cover1999elements}} gives us, an important
result from large deviation theory that also exploits concentration of measure.



Let $\mathbf{x} = (x_1, ..., x_n)$ be a sequence of $n$ each drawn independently from 
a finite universe $U$ with $|U| = m$. Denote by $P_\mathbf{x}$ the empirical distribution – 
or type – for a given sequence $\mathbf{x}$. Let $Q^{n}$ be the product distribution $n$
independent samples of $Q$. 

\begin{theorem}[Sanov’s theorem]\label{sanov}

    Let $\Pi$ be a set of distributions on $U,$ and $m=|U| .$ Let

    \[
        P^{*}=\operatorname{argmin}_{P \in \Pi} D(P \| Q)
    \]
    
    Then
    \[
        \underset{Q^{n}}{\mathbb{P}}\left[P_{\mathbf{x}} \in \Pi\right] \leq(n+1)^{m} 2^{-nD\left(P^{*} \| Q\right)}
    \]
    
\end{theorem}

Applying the above theorem, and noting that $\Gamma_\tau$ takes the place of $\Pi$, $\tilde{B}$ that of $P_{\mathbf{x}}$
and the discretized distribution $\hat{e}_X = X - \hat{f}_X(Y)$ that of $Q$ we get:

\begin{equation}
    \mathbb{P}\left[W \leq \tau \right] = \mathbb{P}\left[ \tilde{B} \in \Gamma_\tau \right] 
    \leq (n+1)^{m} 2^{-nD\left( \tau \right)}
\end{equation}

Where $D\left( \tau \right) := D\left(P^{*} \| Q\right)$, we make the $\tau$ relation excplicit to 
keep in mind that the minimisation is contrainred to the set $\Gamma_\tau$ which depends on $\tau$.

We remark that the only place of concern is if $D\left(P^{*} \| Q\right) = 0$; assuming however that $Q \neq U$, then 
there will be some $\tau$ s.t. $Q \notin \Gamma_\tau$ and thus $D\left(P^{*} \| Q\right) \neq 0$.

We can now conclude by putting everything together; recall that we had shown that we could bound the success probability 
as follows:



\begin{align}
    \mathbb{P}\left[C \leq W\right] &\geq \mathbb{P}\left[C \leq \tau \right] - \mathbb{P}\left[W \leq \tau \right] \\
    &\geq 1 - 2m\exp \left( -2n \frac{\tau^2}{m^2} \right) - (n+1)^{m} 2^{-nD\left( \tau \right)}
\end{align}

This, if we fix $m$, and if there exists some $\tau$ s.t. $D\left( \tau \right) > 0$ then we get consistency
by letting $n \rightarrow \infty$.

We note that to get the best bound we may maximisise the r.h.s. w.r.t. $\tau$.

\newpage
\section{The twin test}


Suppose we are given samples $\mathcal{D} = \{x_i, y_i\}_{i \in [n]}$ from an ANM $X \rightarrow Y$, which recall has the form

\[
    \begin{cases} 
        & Y = f(X) + Z \\
        & X \bigCI Z,\quad X \thicksim p_X,\quad Z \thicksim p_{Z}  
     \end{cases}  
\]

The main strategy of most causal inference methods has been to estimate $f$, and then compute the estimated
residual $\hat{e} = \hat{f}(x) - y$ and then to test the independence between $\hat{e} and X$. This exploits 
the assumption that $X \bigCI Z$; in practice we often have that the noise is \textit{independent}, 
i.e. we produce a sequence $Y_1, ..., Y_n$, where $Z_i \bigCI Z_j$ $\forall i \neq j$.


In the iid setting we can do something more simple -- computationally -- than testing for independence. We 
will illustrate the procedure by an example; We first plot the data $\{x_i, y_i\}_{i \in [n]}$, by plotting
X to Y, and viceversa. 
In the iid setting we can do something more simple -- computationally -- than testing for independence. We 
will illustrate the procedure by an example; We first plot the data $\{x_i, y_i\}_{i \in [n]}$, by plotting
X to Y, and viceversa. 


\begin{figure}[H]
    \captionsetup[subfigure]{labelformat=empty}
    \centering
    \subfloat{{\includegraphics[width=6.5cm]{algo_direct.png} }}%
    % \quad
    \subfloat{{\includegraphics[width=6.5cm]{algo_reverse.png} }}%

    \caption{75 samples of data $X$, $Y$.  The samples are generated independently as follows:
    $y_i = f(x_i) + n_i$ where $x_i$ is drawn from an expoential distribution and $n_i$ is drawn 
    independently from a gaussian one and $f(x) = \tanh(x) + 2\sin(2x) + x^3$}
    \label{fig:algo_data}
\end{figure}

In the iid setting we can do something more simple -- computationally -- than testing for independence. We 
will illustrate the procedure by an example; We first plot the data $\{x_i, y_i\}_{i \in [n]}$, by plotting
X to Y, and viceversa. 

For simplicty assume $X \thicksim \mathcal{U}_{[-a, a]}$ (i.e. $X$ is uniformly disributed), then we can split 
the data in two, say $D_1$ and $D_2$, where we place all samples with $x_i < 0$ into $D_1$, and the rest into 
$D_2$. To be more precise, $\mathcal{D}_1 = \{(x_i, y_i) : x_i < 0\}$ and $\mathcal{D}_2 = \mathcal{D} 
\backslash \mathcal{D}_1$. We also do the same procedure for the reverse set up, i.e. 
$\mathcal{\tilde{D}} = \{y_i, x_i\}_{i \in [n]}$ and by the same procedure obtaining $\mathcal{\tilde{D}}_1$
and $\mathcal{\tilde{D}}_2$. Color coding the different subets, we then have the following:
 
\begin{figure}[H]
    \captionsetup[subfigure]{labelformat=empty}
    \centering
    \subfloat{{\includegraphics[width=6.5cm]{algo_part.png} }}%
    % \quad
    \subfloat{{\includegraphics[width=6.5cm]{algo_part_reverse.png} }}%
    \caption{Samples from two different sources, $X$ and $Y$, how can we tell if 
    they come from the same distribution?}
\end{figure}

If we estimate a fit $\hat{f}_1$ for $\mathcal{D}_1$ and similarly $\hat{f}_2$ for $\mathcal{D}_2$, then we can 
compute residuals for each sets, say $\hat{e}_1$ for $\mathcal{D}_1$  and $\hat{e}_2$ for $\mathcal{D}_2$.
Since the noise is -- not only independent from $X$ but also -- iid, it follows that $\hat{e}_1$ and $\hat{e}_2$
follow the same distribution -- assuming a perfect fit $f$. We can viualize this by looking at the 
histograms from the residuals.

\begin{figure}[H]
    \captionsetup[subfigure]{labelformat=empty}
    \centering
    \subfloat{{\includegraphics[width=6.5cm]{algo_fit_direct.png} }}%
    % \quad
    \subfloat{{\includegraphics[width=6.5cm]{algo_fit_reverse.png} }}%
    % \caption{Samples from two different sources, $X$ and $Y$, how can we tell if 
    % they come from the same distribution?}
    \\

    \subfloat{{\includegraphics[width=6.5cm]{algo_res_direct.png} }}%
    % \quad
    \subfloat{{\includegraphics[width=6.5cm]{algo_res_reverse.png} }}%

    \caption{Example}

\end{figure}

Note that for the reverse model, the noise in $\mathcal{\tilde{D}}_1$ appears to be very different from 
that of $\mathcal{\tilde{D}}_2$; this is not a coincidence -- intuitvely, it seems very unlikely that 
regressing in the other direction will also result in independence noise. Further, as we briefly 
mentioned in the early chapters, as \cite{hoyer2009nonlinear} show, it is unlikely that for a non-linear 
we might not have identifiability. 

One simple idea is then to quantify these observations; from $\mathcal{D}_1$ and $\mathcal{D}_2$ we 
compute $\hat{e}_1$, $\hat{e}_1$, and so we can define as a score for these sets:

$$
     \mathcal{C}(\mathcal{D}_1, \mathcal{D}_2) = \norm{p_1 - p_2}_1
$$

Where $p_1$ is the empirical distribution of $\hat{e}_1$, and similarly for $p_2$ and $\hat{e}_2$. We 
can then apply the score function to $\mathcal{\tilde{D}}_1$ and $\mathcal{\tilde{D}}_2$ and 
infer causality as follows:

\[ 
     \begin{cases} 
        & X \rightarrow Y \quad  \mathcal{C}(\mathcal{D}_1, \mathcal{D}_2) 
        \leq \mathcal{C}(\mathcal{\tilde{D}}_1, \mathcal{\tilde{D}}_2) \\
        & Y \rightarrow X \quad \text{otherwise}
     \end{cases}
\]

Assuming the regressions are \textbf{faithfull}, then as $n \rightarrow \infty$ we know that both 
$p_1$ and $p_2$ will converge to the same $p_Z$ and so $\mathcal{C}(\mathcal{D}_1, \mathcal{D}_2) \rightarrow 0$.
On the other hand, it is unlikely that the residuals of $\mathcal{\tilde{D}}_1$ and $\mathcal{\tilde{D}}_2$ 
follow the same distribution (due to the non-linearlities introduced by f and the additivy of the noise) and 
so we can be pretty confident that asympotically the procedure will correct. In fact, assuming that \textbf{ANM}
$X \rightarrow Y$ is identifiabile will be enough to show that this procedure is consistent.

In essence the algorithm consists of the parts:

\begin{enumerate}
    \item Partition the data
    \item Estimate regressions and residuals
    \item Compute scores
\end{enumerate}

Before giving the general description of the algorithm we will comment on each of these parts.


\subsection{Partition}

Requirements:

dense
disjoint partition
ordering

Disjoint ok

When we partition $\mathcal{D}$ into $\mathcal{D}_1$ and $\mathcal{D}_2$, it is only a good idea to partition
in the center of the mean of $x_i$ if the underlying measure is uniform. However this is rarely the case -- 
not to mention that $Y$ is unlikely to be uniform.

In effect what we require are dense subsets $\mathcal{D}_1$ and $\mathcal{D}_2$ w.r.t. $\mathcal{D}$. This 
will guarantee that if $n \rightarrow \infty$ then so do the sizes of $\mathcal{D}_1$ and $\mathcal{D}_2$.
Since this is in essence a clustering problem, we turn to K-means, perhaps the most popular clustering 
algorithm. 

Show example with more partitions; show example with expoential distrib, partition there. 

If $X$ is indeed unfirom, then the cluster found by K-means will essentially be what our first heuristic method
did (splitting it in the middle). For general distributions, when can we guarantee that the clusters are dense???
It is crucial that the subsets be dense. How to argue?

NEEDS TO BE ADJACENT PARTITIONS

Q: Sai

\subsection{Regression}

A benefit of partitioning the is that we are also partitioning the function we are trying to estimate; in 
particular one would expect that the regression will be easier, e.g. a low order polynomial might be enough.

Show example of many partitions

\subsection{Score functions}

We have seen in previous chapters various ways to measure the distances between two distribtions say 
$p_1$ and $p_2$, via some score function $\mathcal{D}(p_1, p_2)$; for example $\mathcal{D}$ could be the 
MMD metric, or the $l_1$ distance.

Now instead we have a set of distributions, say $P_k = p_1, ..., p_k$, and we wish to see how homogenous
$P_k$ is compared to some other set $\tilde{P}_j$ -- recall that we wish to see in which of the two, 
the sets, the distribtions are more likely to be the same, i.e. we are testing the iid assumption. 

There are several simple ways to go about this:

$$
    C(P_k) = \operatorname{max}_{i, j} \mathcal{D}(p_i, p_j)
$$

Another option is to take an average of the pairwise score:

$$
    C(P_k) = \frac{1}{\binom{k}{2}} \sum_{i < j} \mathcal{D}(p_i, p_j)
$$

or even 

$$
    C(P_k) = \frac{1}{\binom{k}{2}} \sum_{i < j} \mathcal{D}(p_i, p_\mu)
    , \quad p_\mu = \frac{1}{k} \sum_i p_i
$$


Thus if $C(P_k) < C(\tilde{P}_j)$, we can say that the distributions in $P_k$ are
more homogenous; e.g. they more likely to produce similar looking noise. 

\subsection{Algorithm}

We note that the algorithm is a general framework as we are free to choose the partition, regression method
and score function. We would like to point out that we recycle the data, i.e. we use the same data for 
estimating the regression and subsequent score function; obviously if one wishes one can easily split the 
data in the paritions to use a different portion of the data for estimation and for evaluating the score function.

\begin{algorithm}[H]

    \caption{\textbf{Twin method}: General procedure to decide whether $p_{x, y}$ satisfies and ANM $X \rightarrow Y$
        or $Y \rightarrow X$}
  
    \textbf{Input}:

    \begin{enumerate}
        \item I.i.d samples $\mathcal{D} = \{ (x_i, y_i )\}_{i \in [N]}$ of $X$ and $Y$
        \item Partition procedure
        \item Regression method
        \item Score estimator $\hat{C}: E \rightarrow \R$, where E is a set of vectors. 
    \end{enumerate}
    
    \textbf{Output}: $\hat{C}_{X \rightarrow Y}$, $\hat{C}_{Y \rightarrow X}$, dir

    \begin{enumerate}

        \item $\tilde{\mathcal{D}} := \{ ( y_i, x_i )\}_{i \in [N]}$

        \item \textbf{Partition} the data into subsets\footnote{The subsets are disjoint and their union equals the data}:
        \begin{itemize}
            \item[--] $\{ \mathcal{D}_i \}_{i \in [k]} \quad \text{s.t.} \quad \mathcal{D}_i \subset \mathcal{D}, 
            \forall i \in [k]$
            \item[--] $\{ \tilde{\mathcal{D}}_i \}_{i \in [j]} \quad \text{s.t.} \quad \tilde{\mathcal{D}}_i 
            \subset \tilde{\mathcal{D}}, \forall i \in [j]$
            \item[--] Where integers $j, k > 1$ are determined by the partition procedure.
        \end{itemize}

        \item \textbf{Estimate regressions} and residuals for each subset
        
        for $i \in [k]:$

        \begin{itemize}
            \item[--] Let $\mathbf{x}$, $\mathbf{y}$ be the vectors formed from $\mathcal{D}_i$
            \item[--] $\hat{f}_Y$ of the regression function $x \mapsto \E(Y | X=x)$
            \item[--] $ \mathbf{\hat{e}_{Y}}(i) := \mathbf{y} - \hat{f}_Y(\mathbf{x})$
        \end{itemize}

        end for

        $\mathbf{E_Y} := \{ \mathbf{\hat{e}_{Y}}(i) \}_{i \in [k]} $

        for $i \in [j]:$

        \begin{itemize}
            \item[--] Let $\mathbf{x}$, $\mathbf{y}$ be the vectors formed from $\tilde{\mathcal{D}}_i$
            \item[--] $\hat{f}_X$ of the regression function $x \mapsto \E(X | Y=y)$
            \item[--] $ \mathbf{\hat{e}_{X}}(i) := \mathbf{x} - \hat{f}_X(\mathbf{y})$
        \end{itemize}

        end for

        $\mathbf{E_X} := \{ \mathbf{\hat{e}_{X}}(i) \}_{i \in [j]} $

        \item \textbf{Compute scores} to measure the difference between the residuals
        \begin{itemize}
            \item[--] $\hat{C}_{X \rightarrow Y}: = \hat{C}( \mathbf{E_Y} )$ 
            \item[--] $\hat{C}_{Y \rightarrow X}: = \hat{C}( \mathbf{E_X} )$
        \end{itemize}        

        \item Output $\hat{C}_{X \rightarrow Y}$, $\hat{C}_{Y \rightarrow X}$, and
        
        \[ 
        \text{dir} :=  
         \begin{cases} 
            & X \rightarrow Y \quad \text{if} \; \hat{C}_{X \rightarrow Y} \leq \hat{C}_{Y \rightarrow X}\\
            & Y \rightarrow X \quad \text{otherwise}
         \end{cases}
        \]
        
    \end{enumerate}

  \label{alg:twin_test}
  \end{algorithm}


\subsection{Consistency}

prooooff


