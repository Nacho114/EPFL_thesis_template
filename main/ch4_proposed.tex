
\chapter{First principle methods}


In this section we will present our proposed methods for causal inference in the 
bivariate setting : The twin test and the residual method. We will introduce each method, 
provide a detailed algorithmic description and end by giving a proof of correctness. 


\section{The twin test}

Let us recall the setup: 
suppose we are given samples $\mathcal{D} = \{x_i, y_i\}_{i \in [n]}$ from an ANM $X \rightarrow Y$, which has the form

\[
    \begin{cases} 
        & Y = f(X) + Z \\
        & X \bigCI Z,\quad X \thicksim P_X,\quad Z \thicksim P_{Z}  
     \end{cases}  
\]

The main strategy of of the ANM methods has been to estimate $f$, and then to compute the estimated
residual $\hat{\mathbf{e}} = \hat{f}(\mathbf{x}) - \mathbf{y}$; the final step is to test the independence between $\hat{\mathbf{e}}$ and $\mathbf{x}$. This exploits 
the assumption that $X \bigCI Z$. In practice we often have that the noise is \textit{independent} between each sample
i.e. we produce a sequence $Y_1, ..., Y_n$, where $Z_i \bigCI Z_j$ $\forall i \neq j$; for example when we are 
taking measurements, the additive noise of our devices tends to be independent between each sample.

By directly exploiting the i.i.d noise assumption, we will circumvent the need for an independence test. The 
idea is to to partition the data ---  for simplicity you can think about splitting it around the median; we 
then estimate the residuals for each partition, and we then test if the i.i.d noise assumption holds by comparing
the residuals of each partition. We can apply this procedure to both directions and then we will call the 
direction causal if its residuals are more similar. 

We will explain this idea in more detail by following an example: we are given samples 
$\mathcal{D} = \{x_i, y_i\}_{i \in [n]}$ from an ANM $X \rightarrow Y$; 
We first visualize the data $\{x_i, y_i\}_{i \in [n]}$, by plotting
X to Y, and vice versa (see Figure \ref{fig:algo_data}). 

\begin{figure}[H]
    \captionsetup[subfigure]{labelformat=empty}
    \centering
    \subfloat{{\includegraphics[width=6.5cm]{algo_direct.png} }}%
    % \quad
    \subfloat{{\includegraphics[width=6.5cm]{algo_reverse.png} }}%

    \caption{300 samples of data $X$, $Y$. With \textbf{ANM}: \\
    $f(x) = \tanh(x) + 2\sin(2x) + x^3$}
    $X \thicksim \mathcal{U}_{[-a, a]}$ and $Z \thicksim \mathcal{N}(0, \sigma^2)$
    \label{fig:algo_data}
\end{figure}



For simplicity assume $X \thicksim \mathcal{U}_{[-a, a]}$ (i.e. $X$ is uniformly disributed), then we can split 
the data in two\footnote{ The question of how to split the data is important, and we will address this problem in 
more detail later, but for now, given the assumptions, splitting it in half is not such a bad idea; another 
question one might ask is the following: how many intervals? We will clarify all this later.}
, say $D_1$ and $D_2$, where we place all samples with $x_i < 0$ into $D_1$, and the rest into 
$D_2$. To be more precise, $\mathcal{D}_1 = \{(x_i, y_i) : x_i < 0\}$ and $\mathcal{D}_2 = \mathcal{D} 
\backslash \mathcal{D}_1$. We also do the same procedure for the reverse set up, i.e. we reverse the roles of x and y
, $\mathcal{\tilde{D}} = \{y_i, x_i\}_{i \in [n]}$ and by the same procedure we obtain $\mathcal{\tilde{D}}_1$
and $\mathcal{\tilde{D}}_2$. We can visualize this parition bellow in Figure \ref{fig:colo_code}.
 
\begin{figure}[H]
    \captionsetup[subfigure]{labelformat=empty}
    \centering
    \subfloat[Partitions $D_1$ and $D_2$]{{\includegraphics[width=6.5cm]{algo_part.png} }}%
    % \quad
    \subfloat[Partitions $\mathcal{\tilde{D}}_1$ and $\mathcal{\tilde{D}}_2$]{{\includegraphics[width=6.5cm]{algo_part_reverse.png} }}%
    \caption{ We highlight each partition in a different color. On the left we have $D_1$ and $D_2$; and on 
    the right we have $\mathcal{\tilde{D}}_1$ and $\mathcal{\tilde{D}}_2$ }
    \label{fig:colo_code}
\end{figure}

If we estimate a fit $\hat{f}_1$ for $\mathcal{D}_1$ and similarly $\hat{f}_2$ for $\mathcal{D}_2$, then we can 
compute residuals for each sets, say $\hat{e}_1$ for $\mathcal{D}_1$  and $\hat{e}_2$ for $\mathcal{D}_2$.
Since the noise is ---  not only independent from $X$ but also ---  i.i.d, it follows that $\hat{e}_1$ and $\hat{e}_2$
follow the same distribution ---  assuming a perfect fit $f$. We can visualize this by looking at the 
histograms from the residuals.

\begin{figure}[H]
    \captionsetup[subfigure]{labelformat=empty}
    \centering
    \subfloat{{\includegraphics[width=6.5cm]{algo_fit_direct.png} }}%
    % \quad
    \subfloat{{\includegraphics[width=6.5cm]{algo_fit_reverse.png} }}%
    % \caption{Samples from two different sources, $X$ and $Y$, how can we tell if 
    % they come from the same distribution?}
    \\

    \subfloat{{\includegraphics[width=6.5cm]{algo_res_direct.png} }}%
    % \quad
    \subfloat{{\includegraphics[width=6.5cm]{algo_res_reverse.png} }}%

    \caption{  We show the the estimated fits $\hat{f}$ for each parition in black. Bellow each 
    partition we plot the histograms of the residuals ---  in the same color. }
    \label{fig:algo_fit}
\end{figure}

Note that for the reverse model, the noise in $\mathcal{\tilde{D}}_1$ appears to be very different from 
that of $\mathcal{\tilde{D}}_2$; this is not a coincidence ---  intuitively, it seems very unlikely that 
regressing in the other direction will also result in independence noise. Further, as we briefly 
mentioned in the early chapters, as \cite{hoyer2009nonlinear} show, it is unlikely that for a non-linear 
we might not have identifiability. 

One simple idea is then to quantify these observations; from $\mathcal{D}_1$ and $\mathcal{D}_2$ we 
compute $\hat{e}_1$, $\hat{e}_1$, and so we can define as a score for these sets:

$$
     \mathcal{C}(\mathcal{D}_1, \mathcal{D}_2) = \norm{p_1 - p_2}_1
$$

Where $p_1$ is the empirical distribution of $\hat{e}_1$, and similarly for $p_2$ and $\hat{e}_2$. We 
can then apply the score function to $\mathcal{\tilde{D}}_1$ and $\mathcal{\tilde{D}}_2$ and 
infer causality as follows:

\[ 
     \begin{cases} 
        & X \rightarrow Y \quad  \mathcal{C}(\mathcal{D}_1, \mathcal{D}_2) 
        \leq \mathcal{C}(\mathcal{\tilde{D}}_1, \mathcal{\tilde{D}}_2) \\
        & Y \rightarrow X \quad \text{otherwise}
     \end{cases}
\]

In the above example, we get that $\mathcal{C}(\mathcal{D}_1, \mathcal{D}_2) = 0.138$ and that
$\mathcal{C}(\mathcal{\tilde{D}}_1, \mathcal{\tilde{D}}_2) = 0.480$ where use bins of size $5$ 
for discretization; We are able to predict the causal direction with high confidence.

Assuming the regressions are \textbf{suitable}, then as $n \rightarrow \infty$ we know that both 
$p_1$ and $p_2$ will converge to the same $p_Z$ and so $\mathcal{C}(\mathcal{D}_1, \mathcal{D}_2) \rightarrow 0$.
On the other hand, it is unlikely that the residuals of $\mathcal{\tilde{D}}_1$ and $\mathcal{\tilde{D}}_2$ 
follow the same distribution (due to the non-linearlities introduced by f) and 
so we can be pretty confident that asymptotically the procedure will correct. In fact, assuming that \textbf{ANM}
$X \rightarrow Y$ is identifiable will be enough to show that this procedure is consistent.

In essence the algorithm consists of the parts:

\begin{enumerate}
    \item Partition the data
    \item Estimate regressions and residuals for each partition
    \item Compute scores between partition
\end{enumerate}

We note that the algorithm is a general framework as we are free to choose the partition, regression method
and score function. As in the ANM methods, one can either form a train/test split to learn the regression, or
instead recycle the data. The algorithmic description can be found in Algorithm \ref{alg:twin_test}.

We will next describe in more detail the core parts of the algorithm.

\subsection{Partition}

Say that we partition\footnote{
    A partition is said disjoint if:
    $\mathcal{D} = \bigcup_i \mathcal{D}_i $ and $\mathcal{D}_i \cap \mathcal{D}_j = \emptyset \quad \forall i \neq j$}
$\mathcal{D}$ into disjoint sets $\mathcal{D}_1, ..., \mathcal{D}_k$; then these 
partitions need to satisfy two requirements:

\begin{enumerate}
    \item The partitions need to be \textbf{dense}: 
    $|\mathcal{D}_i| \geq \rho |\mathcal{D}|, \quad \forall i \in [k]$, for some $\rho \in (0, 1)$.
    % \item By disjoint partitions we mean that
    % $$
    % \mathcal{D} = \bigcup_i \mathcal{D}_i \quad \text{and} \quad
    % \mathcal{D}_i \cap \mathcal{D}_j = \emptyset \quad \forall i \neq j
    % $$
    \item We need to be able to order the partitions, say $\mathcal{D}_1, ..., \mathcal{D}_k$, such that
    if $i < j$ then\footnote{The $*$ is to indicate a dummy variable, as we do not care for the value of $y$.}: 
    $$
    \operatorname{max} \{ x : (x, *) \in \mathcal{D}_i \} \leq 
    \operatorname{min} \{ x : (x, *) \in \mathcal{D}_j \}
    $$
\end{enumerate}

The first condition ---  that of dense partitions ---  is to avoid getting trivial large deviations between residuals 
in the subsets; the second reason is that if they are dense, then we can give asymptotic guarantees about each subset.
The second conditions simply ensure that we are not mixing data and that it is coherent to make regression
in each subset. 

If we use K-means (perhaps the most popular clustering algorithm), then condition 2 are met. The only 
questions is in regards to conditions 1. K-means starts by randomly initializing two or n centers (depending 
on the number of clusters that we want), and the updates the centers that they locally 
minimizes within-cluster variances. If our data is infinite support, and we re-run K-means if there is some cluster
$i$ s.t. $|\mathcal{D}_i| < \rho |\mathcal{D}|$; then if we have enough data and for some $\rho$ we can be quite 
certain that the algorithm will eventually terminate. 

In practice this has always been the case (for $\rho = .3$); so we conjecture that one can prove the above statement 
(or some variant) rigorously. More formally, given $n$ samples $ \mathcal{D} = x_1, ..., x_n$ from some random variable $X$ 
that is "well behaved"\footnote{By well behaved it would relate to concentration, if the data has enough spread, then 
partitioning should occur --- versus being concentrated at one point.}, by running k-means once we get two partitions, 
$\mathcal{D}_1$ and $\mathcal{D}_2$; we can
restart k-means $r$  times with a different initialization to obtain $\mathcal{D}_1(1), ...\mathcal{D}_1(r)$ and 
$\mathcal{D}_2(1), ...\mathcal{D}_2(r)$. Let $s_j = \operatorname{min} (\mathcal{D}_1(j), \mathcal{D}_2(j))$

then there is some $\rho > 0$ s.t.

$$
    \Prob \left( \operatorname{min}_j s_j \geq \rho n \right) \rightarrow 1
$$

as $n \rightarrow \infty$, with $r = \log(n)$. 

The last question is, "How many clusters do we want?". Obviously for the small data regime we must be content 
with only two clusters; but what if we have a lot of data? Experimentally we observed that if we 
we choose the number of partition as an increasing function w.r.t. sample size, then we can get better performance.

One crucial aspect is that we work in the 1-D setting, where clustering is of reasonable difficulty; in higher dimensions 
clustering will not be so easy. 


\subsection{Regression}

A benefit of partitioning the is that we are also partitioning the function we are trying to estimate; in 
particular one would expect that the regression will be easier, e.g. a low order polynomial might be enough.

For regression we try two different methods:

\begin{enumerate}
    \item Polynomial regression: We perform model selection based on the BIC\footnote{
    The BIC score is defined as follows: $\mathrm{BIC} =k\ln(n)-2\ln({\widehat{L}})$; where $\widehat{L}$
    is the the maximized value of the likelihood function of the model, $n$ the number of samples and 
    $k$ the number of parameters in model. In essence it is a score that 
    trades off model fit with model complexity. } score. We pick the model with the 
    best BIC score with degree at most 6. 
    \item Neural networks: We perform regression with a one layer 100 neuron network with ReLu activation
    function, and trained on Adam.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=.45\textwidth]{partition_smooth.png}
    \caption{An example of how partitioning the data may make the regression task easier.}
\end{figure}


\subsection{Score functions}

We have seen in previous chapters various ways to measure the distances between two distributions say 
$p_1$ and $p_2$, via some score function $d(p_1, p_2)$; for example $d$ could be the 
MMD metric, or the $l_1$ distance.

Now instead we have a set of distributions, say $P_k = \{p_1, ..., p_k\}$, and we wish to see how homogenous\footnote{
    By homogenous set we simply mean one in which its elements resemble each other, which is precisely what we will try to measure. 
}
$P_k$ is compared to some other set $\tilde{P}_j$ ---  recall that we wish to see in which of the two, 
 the distributions are more likely to be the same, i.e. we are testing the i.i.d assumption. 

There are several simple ways to go about this:

$$
    C(P_k) = \operatorname{max}_{i, j} d(p_i, p_j)
$$

Another option is to take an average of the pairwise score:

$$
    C(P_k) = \frac{1}{\binom{k}{2}} \sum_{i < j} d(p_i, p_j)
$$

or even 

$$
    C(P_k) = \frac{1}{\binom{k}{2}} \sum_{i < j} d(p_i, p_\mu)
    , \quad p_\mu = \frac{1}{k} \sum_i p_i
$$


Thus if $C(P_k) < C(\tilde{P}_j)$, we can say that the distributions in $P_k$ are
more homogenous; e.g. they more likely stem from the same noise distribution. 

We have tested all of the above and find that the first method ---  using the maximum score between pairs ---
gives the best performance.




\subsection{Proof of consistency}

We will show consistency for a simple set up of the twin test ---  but we note that generalizing it to the more 
general case should follow with little effort from our proof. We leave it as an exercise for the reader. 


The setup was the linear \textbf{ANM}:

\[ \begin{cases} 
    & Y = f(X) + Z  \\
    & X \bigCI Z,\; X \thicksim p_x,\; Z \thicksim p_{Z}  
 \end{cases}
\]

In practice we are given samples $\mathcal{D} = \{x_i, y_i\}_{i \in [n]}$ from an ANM $X \rightarrow Y$; next 
the algorithm will proceed to split the data into sets $\mathcal{D}_1$ and $\mathcal{D}_2$. It then proceeds 
to compute residuals and to compute some scores between them.

To simplify the proof, we will \textbf{skip the partition procedure} and assume that
we are directly given $\mathcal{D}_1$ and $\mathcal{D}_2$, each with 
$n$ samples (Note that, if $p_x$ is uniform, then having these sets be of equal size would happen exponentially 
fast; indeed, in general, we will have dense partitions). 

Next, we will assume that on each interval, the data is linear with slope $a_1$ and $a_2$ resp. 

\begin{figure}[!h]
    \centering
    \begin{tikzpicture}
        \draw[thin, dashed] (1.5, -0.5) --  (1.5, 2.6) node[above] {};
        \path (0.75, -0.8) -- (0.75, -0.6) node[above] {$\mathcal{D}_1$};
        \path (2.25, -0.8) -- (2.25, -0.6) node[above] {$\mathcal{D}_2$};
        \draw[->] (-1, 0) -- (4.2, 0) node[right] {$x$};
        \draw[->] (0, -1) -- (0, 3) node[above] {$y$};
        \draw[scale=1, domain=0:1.5, smooth, variable=\x, blue] plot ({\x}, {0.5*\x});
        \draw[scale=1, domain=1.5:3, smooth, variable=\y, red]  plot ({\y}, {1*\y - 0.75});
        \draw[decoration={brace,mirror,raise=5pt},decorate] (0.1, -.5) -- node[below=8pt] {$\mathcal{D}$} (2.9, -.5) ;
      \end{tikzpicture}
      \caption{Slopes $a_1$ and $a_2$ in blue and red respectively.}
\end{figure}

Thus our problem can be seen as getting data from two difference \textbf{ANM}, both with identical noise, but with 
a different truncation of $X$:

$\mathcal{D}_1$ is sampled from 

\[ \begin{cases} 
    & Y_1 = a_1 X_1 + Z  \\
    & X_1 \bigCI Z,\; X_1 \thicksim p_{X_1},\; Z \thicksim p_{Z}  
 \end{cases}
\]

and $\mathcal{D}_2$ is sampled from 

\[ \begin{cases} 
    & Y_2 = a_2 X_2 + Z  \\
    & X_2 \bigCI Z,\; X_2 \thicksim p_{X_2},\; Z \thicksim p_{Z}  
 \end{cases}
\]

Without loss of generality we can assume $X_1 \thicksim X_2 \thicksim X \thicksim P_X$.

We will call this scenario the \textbf{simplified Twin Test scenario}. 

We next describe the steps of the algorithm after partitioning:

We first split $\mathcal{D}_1$ in two sets of equal size $\mathcal{D}_{1}^{train}$ and 
$\mathcal{D}_{1}^{test}$. We first use $\mathcal{D}_{1}^{train}$ to estimate $a_1$, say $\hat{a}_1$ via 
regression. Then, using $\mathcal{D}_{1}^{test}$, we estimate the residual:

$$
    \hat{Z}_1 = Y_1 - \hat{a}_1 X
$$

We then discretize $\hat{Z}_1$ and form a distribution say $\hat{p}_1$; we can do the same thing for 
$\mathcal{D}_2$, and by doing the same procedure obtain $\hat{p}_2$.

We discretize both with a fixed step size, say $s$; as we will see, the only requirement is that we fix 
the size beforehand.

We use the $l_1$ distance as our score:

$$
    \hat{C}_{X \rightarrow Y} = \norm{\hat{p}_1 - \hat{p}_2}_1
$$

As we have seen, $\hat{C}_{Y \rightarrow X} > 0$ holds in general except in very particular situations. So, 
to prove that our algorithm is consistent, we need to show that:

$$
    \hat{C}_{X \rightarrow Y} \rightarrow 0 \qquad \text{as }  \qquad n \rightarrow \infty
$$

This is precisely what we will show:

\begin{theorem}
    The \textbf{simplified Twin Test scenario} is consistent, i.e. 

$$
    \hat{C}_{X \rightarrow Y} \rightarrow 0 \qquad \text{as }  \qquad n \rightarrow \infty
$$

Assuming that the noise distribution satisfies the following: 
$\int \left| P_{Z}^{(n)}(t) \right| d t < L$, $\forall n\geq 1$, for some $L > 0$
\end{theorem}

The assumption about the noise distribution holds for most distributions, for example uniform, 
normal, exponential, ...

The idea of the proof is to observe the following:

If we have enough data, i.e. when $n$ is large enough then assuming the regression is 
\textbf{suitable}, we can chose any $\alpha$ such that

$$
    \abs{a_1 - \hat{a}_1} \leq \alpha \qquad \text{and} \qquad \abs{a_2 - \hat{a}_2} \leq \alpha
$$

This means that 

$$
    \hat{Z}_1 = Z + (a_1 - \hat{a}_1)X \quad \implies \quad Z - \alpha X \leq \hat{Z}_1 \leq Z + \alpha X 
$$

and similarly

$$
    \hat{Z}_2 = Z + (a_2 - \hat{a}_2)X \quad \implies \quad Z - \alpha X \leq \hat{Z}_2 \leq Z + \alpha X 
$$

Note that $\hat{Z}_1 \thicksim P_{Z + \Delta_1 X}$ and $\hat{Z}_2 \thicksim P_{Z + \Delta_2 X}$ , where for 
brevity we denote $\Delta_1 = a_1 - \hat{a}_1$ and $\Delta_2 = a_2 - \hat{a}_2$. We can visualize the distance
between these distributions as follows: (see\footnote{Note that the illustration does not follow the actual
geometry of the space, we draw it solely to gain intuition about the problem.} figure \ref{fig:dist})

\begin{figure}[!h]
    \centering
    \begin{tikzpicture}[scale=2]
        \draw[thick] (0,0) circle (1);
        \draw[semithick, fill=black] (0,0) circle (0.02) node[left] {$P_Z$};
        \draw[thick, <->] (0, .05) -- node[right] {$R(\alpha)$} (0, .95);
        \draw[semithick, fill=black] (0,1) circle (0.02) node[above] {$P_{Z + \alpha X}$};
        % p_1 and p_2
        \draw[semithick, fill=black] (.5, -.5) circle (0.02) node[above, color=red] {$P_{Z + \Delta_2 X}$};;
        \draw[semithick, fill=black] (-.58, .114) circle (0.02) node[above, color=blue] {$P_{Z + \Delta_1 X}$};;
      \end{tikzpicture}
      \caption{The $l_1$ "ball" around $P_Z$, for brevity we denote 
      $\Delta_1 = a_1 - \hat{a}_1$ and $\Delta_2 = a_2 - \hat{a}_2$}
      \label{fig:dist}
\end{figure}

The idea is then the following, given some $\epsilon > 0$, we want to show that asymptotically

$$
    \norm{\hat{p}_1 - \hat{p}_2}_1 > \epsilon
$$

cannot happen. The game plan will be to find a bound on the $R(\alpha)$; once we have one, we are done, we 
will simply pick an $\alpha$ s.t. $R(\alpha) < \epsilon$. Recall that $\alpha$ is the error in of our $\hat{a}$
estimate, which we can get arbitrarily small with enough samples. 

We begin by proving lemmas to find bounds for the radius $R(\alpha)$.

\begin{lemma} 
    
    Given $\alpha > 0$, random variables $Z$ and $X$ , with $X \bigCI Z$, 
$X \thicksim P_X$, $Z \thicksim P_Z$, such that\footnote{
    we remark that this 
    condition holds for most distributions such as the uniform, exponential and Gaussian.} there is 
    some $g$ s.t. 
    $ \forall \theta \in (-\sqrt{\alpha}, \sqrt{\alpha}), \; \left| P_{Z}^{\prime}(t - \theta) \right| \leq g(t) \; \forall t$
    and $\int \left| g(t) \right| d t < L$, for some $L > 0$.
 then

$$
\norm{P_{Z}- P_{\alpha X + Z}}_{1} \leq \sqrt{\alpha} L + 2 \Prob \left(\abs{ X} > \frac{1}{\sqrt{\alpha}} \right)
$$

Where $P_{\alpha X + Z}$ is the distribution of the sum: $\alpha X + Z \thicksim  P_{\alpha X + Z}$.

\label{lemma:conv_bound}
\end{lemma}

 

\begin{proof}
    ~

First note that $\alpha X \thicksim \frac{1}{\alpha}P_{X} \left( \frac{\tau}{\alpha} \right)$ by applying the 
change of variable rule. Let $P_{\alpha X + Z}$ be the distribution of the sum: $\alpha X + Z \thicksim  P_{\alpha X + Z}$.

Next, since $X \bigCI Z$, we may write $P_{\alpha X + Z}$ as a convolution:

$$
    P_{\alpha X + Z} (t) = \int P_{Z}(t-x) \frac{1}{\alpha}P_{X} \left( \frac{x}{\alpha} \right) d x =
     \int P_{Z}(t-\alpha x) P_{X}(x) d x
$$

Let $T^{\alpha X} P_Z(t) := P_{Z}(t-\alpha x)$ (we use the notation introduced by Lagrange for the shift operator).

Hence we may write (with $P_X$ as the underlying measure):

$$
    P_{\alpha X + Z} (t) = \E\left(P_{Z}(t-\alpha X ) \right)
$$


We proceed as follows:

$$
\begin{aligned}
\norm{P_{Z}- P_{\alpha X + Z}}_{1} &=\int\abs{ P_{Z}(t)-\E\left(P_{Z}(t-\alpha X ) \right)} d t \\
&\leq \int \E \abs{ P_{Z}(t)-P_{Z}(t-\alpha X )} d t \\
&=\E \int\abs{ P_{Z}(t)-P_{Z}(t-\alpha X ) } d t \\
&= \E \left( \norm{P_{Z}- T^{\alpha X} P_Z }_{1} \, | \, \abs{X} \leq \frac{1}{\sqrt{\alpha}} \right) 
\Prob\left( \abs{X} \leq \frac{1}{\sqrt{\alpha}} \right) + \E \left( \norm{P_{Z}- T^{\alpha X} P_Z }_{1} \, | \, \abs{X} > \frac{1}{\sqrt{\alpha}} \right) 
\Prob\left( \abs{X} > \frac{1}{\sqrt{\alpha}} \right) \\
&\leq \E \left( \norm{P_{Z}- T^{\alpha X} P_Z }_{1} \, | \, \abs{X} \leq \frac{1}{\sqrt{\alpha}} \right) 
 + 2 \Prob \left(\abs{ X} > \frac{1}{\sqrt{\alpha}} \right) \\ 
&\leq \operatorname{max}_{\eta \in (-\sqrt{\alpha}, \sqrt{\alpha})} \int\abs{ P_{Z}(t)-P_{Z}(t-\eta ) } d t + 2 \Prob \left(\abs{ X} > \frac{1}{\sqrt{\alpha}} \right) \\
&\leq \operatorname{max}_{\eta \in (-\sqrt{\alpha}, \sqrt{\alpha})} \int \abs{ \eta g(t) } d t  + 2 \Prob \left(\abs{ X} > \frac{1}{\sqrt{\alpha}} \right)\\
% &\leq \int\abs{ P_{Z}(t)- \left( P_{Z}(t) + \sum_{n \geq 1} \left( \alpha C^* \right)^n P_{Z}(t)^{(n)} \right) } d t  + 2 P(\abs{ X} > \frac{1}{\sqrt{\alpha}})\\
% &\leq \alpha C^* L + 2 P(\abs{ X} > \frac{1}{\sqrt{\alpha}})\\
&\leq \sqrt{\alpha} \int \abs{ g (t)} dt + 2 \Prob \left(\abs{ X} > \frac{1}{\sqrt{\alpha}} \right)\\
&\leq \sqrt{\alpha} L + 2 \Prob \left(\abs{ X} > \frac{1}{\sqrt{\alpha}} \right)
\end{aligned}
$$

The first equality follows by the aforementioned observation. The first inequality follows from the triangle 
inequality; 
the equality that comes after is due to Fubini's theorem, we can swap the expectation (which is also an 
integration) since all measures are measurable. We next use the law of total probability by splitting 
the expectation w.r.t $\frac{1}{\sqrt{\alpha}}$.

The next upper bounds follows by noting that
$\norm{p - q}_1 \leq 2$ for any distributions $p$ and $q$. Next observe that 
$\E \left( \norm{P_{Z}- T^{\alpha X} P_Z }_{1} \, | \, \abs{X} \leq \frac{1}{\sqrt{\alpha}} \right)$ is the average $l_1$ distance
between $\norm{P_{Z}}$ and random shifts of itself. Hence we can bound this by the maximum shift.

Since $\eta \approx 0$, and since 
$  \eta \in (-\sqrt{\alpha}, \sqrt{\alpha}) \implies  \left| P_{Z}^{\prime}(t - \eta) \right| \leq g(t) \; \forall t$
, using Taylor's theorem we obtain that: 

$$
    \abs{P_{Z}(t- \eta ) - P_{Z}(t) }  \leq \eta g(t)
$$

The rest follows from the assumption that $\int g(t) dt \leq L$.

% % Using taylor expansions we obtain 

% % $$
% %     P_{Z}(t-\alpha C^* ) = P_{Z}(t) + \sum_{n \geq 1} \left( \alpha C^* \right)^n P_{Z}(t)^{(n)}
% % $$

% % then  we upper bound what we get after as follows

% % \begin{align*}
% %     \int \left| \sum_{n \geq 1} \left( \alpha C^* \right)^n P_{Z}(t)^{(n)} \right|
% %     &\leq L \sum_{n \geq 1} \left( \alpha C^* \right)^n \\
% %     &\leq L \frac{\alpha C^*}{1 - \alpha C^*} \\
% %     &\approx \alpha C^* L
% % \end{align*}

% % We will later pick $\alpha \ll 1$, and so it follows from $\frac{x}{1 - x} \approx x$ for small $x$. 

% Finally, since are free to pick $k$, we choose it large enough s.t. $2P(\abs{ X} > k) \leq \delta$, we then let 
% $C = C^* L$ and we are done.

% Note that as $k$ increases so does $C^*$, since we are growing the space to optimise for $C^*$; this is 
% where the dependence between $\delta$ and $C$ comes from. 

\end{proof}

So we have found a bound on the $l_1$ distance between two continuous distributions; however in our
application, these will be quantized versions of these distributions. The following lemma tells us 
that this is not a problem, the $l_1$ distance between the quantized version cannot be bigger than 
that of their continuous counter parts. The only requirement is that we fix the quantization scheme
beforehand and use the same one for both.

\begin{lemma}
    Let $P$ and $Q$ be two continuous distributions, then let $P^*$, $Q^*$ resp. be discretized versions.
    Then

    $$
        \norm{P^* - Q^*}_1 \leq \norm{P - Q}_1 
    $$
    \label{lemma:quantisation}
\end{lemma}

\begin{proof}
    ~
    We first quantize $\R$ in bins of length $w$, say $I_i = [wi, w(i + 1))$, note 
    $\bigcup_{i \in \Z} I_i = \R$.

    Given continuous distributions $P$ and $Q$, we form their quantized counter parts as
    follows:

    $$
        P^*(k) := \sum _{i \in \Z} \; \int_{I_i} P(t) dt \; \Ind_{k  = i}
        , \quad   Q^*(k) := \sum _{i \in \Z} \; \int_{I_i} Q(t) dt \; \Ind_{k  = i}
    $$

    We then conclude as follows by a applying the triangle inequality twice:

    \begin{align*}
        \norm{P^* - Q^*}_1^2 &= \sum _{k \in \Z} \; \left| P^*(k) - Q^*(k)\right|  \\
        &\leq \sum _{k \in \Z} \sum _{i \in \Z} \; \left| \int_{I_i} \left(P(t) - Q(t)\right) dt \right|  \; \Ind_{k  = i} \\
        &\leq \sum _{k \in \Z} \sum _{i \in \Z} \;  \int_{I_i} \left|P(t) - Q(t)\right| dt  \; \Ind_{k  = i} \\
        &= \sum _{i \in \Z} \;  \int_{I_i} \left|P(t) - Q(t)\right| dt  \; \sum _{k \in \Z} \Ind_{k  = i} \\
        &= \sum _{i \in \Z} \;  \int_{I_i} \left|P(t) - Q(t)\right| dt \\
        &= \int_{\R} \left|P(t) - Q(t)\right| dt \\
        &= \norm{P - Q}_1^2
    \end{align*}

\end{proof}

We can now conclude by proving consistency, recall that we want to show that:

$$
    P \left(\norm{\hat{p}_1 - \hat{p}_2}_1 > \epsilon \right) \rightarrow 0
$$

\begin{proof}

\begin{align*}
        \Prob\left( \norm{ \hat{p}_1 - \hat{p}_2 }_1 \geq \epsilon \right) &\leq 
        \Prob\left( \norm{ \hat{p}_1 -  P^*_Z}_1 + \norm{ \hat{p}_2 -  P^*_Z}_1 \geq \epsilon \right) \\
        &\leq \Prob\left( \norm{ \hat{p}_1 -  P^*_Z}_1 + \norm{ \hat{p}_2 -  P^*_Z}_1 \geq \epsilon 
        \; | \; \abs{a_1 - \hat{a}_1} \leq \alpha \; \text{,} \; \abs{a_2 - \hat{a}_2} \leq \alpha \right) \\
                        &\qquad + \Prob\left( \abs{a_1 - \hat{a}_1} > \alpha \; \text{or} \; \abs{a_2 - \hat{a}_2} > \alpha  \right) 
\end{align*}

Where $P^*_Z$ is the quantized version of $P_Z$.

The first inequality follows by the triangle inequality, and the second one by using the law of total probability.

Observe that 

$$
\Prob\left( \abs{a_1 - \hat{a}_1} > \alpha \; \text{or} \; \abs{a_2 - \hat{a}_2} > \alpha  \right)  =
\Prob\left( \abs{a_1 - \hat{a}_1} > \alpha \right) + P \left( \abs{a_2 - \hat{a}_2} > \alpha  \right) 
$$

Both of which go to zero for any $\alpha > 0$ assuming that our regression is \textit{suitable}.


It remains to bound 

\begin{equation}
    \Prob\left( \norm{ \hat{p}_1 -  P^*_Z}_1 + \norm{ \hat{p}_2 -  P^*_Z}_1 \geq \epsilon 
        \; | \; \abs{a_1 - \hat{a}_1} \leq \alpha \; \text{,} \; \abs{a_2 - \hat{a}_2} \leq \alpha \right)
\end{equation}

Note that $\hat{p}_1 \rightarrow P^*_{Z + \Delta_1 X}$ as $n \rightarrow \infty$; where 
$P^*_{Z + \Delta_1 X}$ is the discretized distribution of $P_{Z + \Delta_1 X}$; recall that 
we form $\hat{p}_1$ by creating a discretized histogram from the residuals. 

Then, by combing lemma \ref{lemma:conv_bound} and \ref{lemma:quantisation} we have that:

For any $\alpha$, there is some $L > 0$ s.t.
$$
    \norm{P^*_{Z} - P^*_{Z + \Delta_1 X}}_1 \leq \sqrt{\alpha} L + 2 \Prob \left(\abs{ X} > \frac{1}{\sqrt{\alpha}} \right)
$$



Thus if we are given some $\epsilon > 0$, pick\footnote{We can do this since $\sqrt{\alpha}$ is increasing 
in $\alpha$ and $\Prob \left(\abs{ X} > \frac{1}{\sqrt{\alpha}} \right)$ is decreasing in $\alpha$.} 
some $\alpha$ such that

$$
    \sqrt{\alpha} L + 2 \Prob \left(\abs{ X} > \frac{1}{\sqrt{\alpha}} \right) < \frac{\epsilon}{2}
$$

By applying the same idea to $\hat{p}_2$, we get that as $n \rightarrow \infty$

$$
   \norm{\hat{p}_1 -  P^*_Z}_1 + \norm{ \hat{p}_2 -  P^*_Z}_1 < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
$$

And so as $n \rightarrow \infty$

$$
    \Prob\left( \norm{ \hat{p}_1 -  P^*_Z}_1 + \norm{ \hat{p}_2 -  P^*_Z}_1 \geq \epsilon 
        \; | \; \abs{a_1 - \hat{a}_1} \leq \alpha \; \text{,} \; \abs{a_2 - \hat{a}_2} \leq \alpha \right)
        \rightarrow 0 
$$


\end{proof}


\subsection{Algorithm}


\begin{algorithm}[H]

    \caption{\textbf{Twin method}: General procedure to decide whether $P_{X, Y}$ satisfies and ANM $X \rightarrow Y$
        or $Y \rightarrow X$}
  
    \textbf{Input}:

    \begin{enumerate}
        \item I.i.d samples $\mathcal{D} = \{ (x_i, y_i )\}_{i \in [N]}$ of $X$ and $Y$
        \item Partition procedure
        \item Regression method
        \item Score estimator $\hat{C}: R^{* \times *} \rightarrow \R$, where E is a set of vectors. 
    \end{enumerate}
    
    \textbf{Output}: $\hat{C}_{X \rightarrow Y}$, $\hat{C}_{Y \rightarrow X}$, dir

    \begin{enumerate}

        \item $\tilde{\mathcal{D}} := \{ ( y_i, x_i )\}_{i \in [N]}$

        \item \textbf{Partition} the data into subsets\footnote{The subsets are disjoint and their union equals the data}:
        \begin{itemize}
            \item[--] $\{ \mathcal{D}_i \}_{i \in [k]} \quad \text{s.t.} \quad \mathcal{D}_i \subset \mathcal{D}, 
            \forall i \in [k]$
            \item[--] $\{ \tilde{\mathcal{D}}_i \}_{i \in [j]} \quad \text{s.t.} \quad \tilde{\mathcal{D}}_i 
            \subset \tilde{\mathcal{D}}, \forall i \in [j]$
            \item[--] Where integers $j, k > 1$ are determined by the partition procedure.
        \end{itemize}

        \item \textbf{Estimate regressions} and residuals for each subset
        
        for $i \in [k]:$

        \begin{itemize}
            \item[--] Let $\mathbf{x}$, $\mathbf{y}$ be the vectors formed from $\mathcal{D}_i$
            \item[--] $\hat{f}_Y$ of the regression function $x \mapsto \E(Y | X=x)$
            \item[--] $ \mathbf{\hat{e}_{Y}}(i) := \mathbf{y} - \hat{f}_Y(\mathbf{x})$
        \end{itemize}

        end for

        $\mathbf{E_Y} := \{ \mathbf{\hat{e}_{Y}}(i) \}_{i \in [k]} $

        for $i \in [j]:$

        \begin{itemize}
            \item[--] Let $\mathbf{x}$, $\mathbf{y}$ be the vectors formed from $\tilde{\mathcal{D}}_i$
            \item[--] $\hat{f}_X$ of the regression function $y \mapsto \E(X | Y=y)$
            \item[--] $ \mathbf{\hat{e}_{X}}(i) := \mathbf{x} - \hat{f}_X(\mathbf{y})$
        \end{itemize}

        end for

        $\mathbf{E_X} := \{ \mathbf{\hat{e}_{X}}(i) \}_{i \in [j]} $

        \item \textbf{Compute scores} to measure the difference between the residuals
        \begin{itemize}
            \item[--] $\hat{C}_{X \rightarrow Y}: = \hat{C}( \mathbf{E_Y} )$ 
            \item[--] $\hat{C}_{Y \rightarrow X}: = \hat{C}( \mathbf{E_X} )$
        \end{itemize}        

        \item Output $\hat{C}_{X \rightarrow Y}$, $\hat{C}_{Y \rightarrow X}$, and
        
        \[ 
        \text{dir} :=  
         \begin{cases} 
            & X \rightarrow Y \quad \text{if} \; \hat{C}_{X \rightarrow Y} \leq \hat{C}_{Y \rightarrow X}\\
            & Y \rightarrow X \quad \text{otherwise}
         \end{cases}
        \]
        
    \end{enumerate}

  \label{alg:twin_test}
  \end{algorithm}

\newpage
\section{The residual method}

When given an ANM $X \rightarrow Y $, the traditional method based on ANM is to regress $X$ on 
$Y$ and then vice versa in order to see which residual is more independent from it's input. A
very basic --- but restrictive --- idea is to assume knowledge about the additive noise, $P_Z$. 
In some sense this idea was the precursor to the twin test, where we check if the noise is consistent
in the different intervals of the data. Instead here, since we assume knowledge about $P_Z$, we 
will test and see which residual is more likely to be drawn from the actual noise distribution $P_Z$.

This is however quite a strong assumption, so it is more of a theoretical curiosity. 

We start again we typical setup:
suppose we are given samples $\mathcal{D} = \{x_i, y_i\}_{i \in [n]}$ from an ANM $X \rightarrow Y$, which has the form

\[
    \begin{cases} 
        & Y = f(X) + Z \\
        & X \bigCI Z,\quad X \thicksim P_X,\quad Z \thicksim P_{Z}  
     \end{cases}  
\]

In addition, we will know $P_{Z}$, in some cases it is not such an unreasonable assumption; for example, 
a lot of thermal noise in measurements is usually very well modeled by a Gaussian. Another example 
is when a real value x from a sensor, is discretized with a uniform quantizer, the error is 
likely to be uniformly distributed. (\cite{uniformCond})

For the ANM methods, the first step was to 

1. Regress $\mathbf{x}$ on $\mathbf{y}$, to find an estimate say $\hat{f}_Y$

2. Estimate residual via $\mathbf{\hat{e}}_Y = \hat{f}_Y(\mathbf{x}) - \mathbf{y}$

The next step was to then compute the same thing for the reverse model, that is, swapping the 
roles of $\mathbf{x}$ and $\mathbf{y}$. We would then compute some score of independence 
between the residual and their respective inputs ($\mathbf{x}$ for the direct model And
 $\mathbf{y}$ for the reverse). 

 Since we have knowledge of $P_Z$ we can avoid this last step all together, and instead simply 
 compute a score to see how close $\mathbf{\hat{e}}_Y$ is to $P_Z$. 

 One simple idea is the following:

 1. $\mathbf{b} :=$ histogram of $\mathbf{\hat{e}}_Y$

 2. $P^*_Z :=$  discretized distribution of $P_Z$

 For both we pick the same discretization size, say $m$. 

 Our score is then 
 $
     \hat{C}_{X \rightarrow Y} := d\left(\mathbf{b}, P^*_Z \right)
 $
 Where $d$ is some statistical distance, such as an f-divergence. We compute the score for reverse model 
 by swapping the roles of $\mathbf{x}$ and $\mathbf{y}$.


\subsection{Proof of consistency: A tale of two bounds}

The setup was the linear \textbf{ANM}:

\[ \begin{cases} 
    & Y = aX + Z  \\
    & X \bigCI Z,\; X \thicksim P_X,\; Z \thicksim P_{Z}  
 \end{cases}
\]

Given data $\mathcal{D} = \{x_i, y_i\}_{i \in [n]}$ from this ANM, we 
estimate $\hat{f}_Y$ by regressing $X$ on $Y$ and 
$\hat{f}_X$ for the reverse model. We then compute the residuals (as always we can 
either recycle data or perform a test/train partition). 

\begin{align}
    &  \hat{\mathbf{e}}_Y = \mathbf{y} - \hat{f}_Y(\mathbf{x})\\
    &  \hat{\mathbf{e}}_X = \mathbf{x} - \hat{f}_X(\mathbf{y})
\end{align}

We note that for the ease of analysis, it would first be wise to use some fraction 
of the data to first estimate the regression, and then use the remaining for the test.

The idea is the very simple, test which of the residuals $\hat{\mathbf{e}}_Y$ or 
$\hat{\mathbf{e}}_X$ is more likely to be distributed according to $P_Z$.
To ease computation in the bounds we will assume $P_Z$ to be the uniform
distribution, but we remark that the analysis will hold in general. 

To do so we first discretize\footnote{We do so in a naive manner by splitting
it uniformly into $m$ bins.} $P_{Z}$ into $m$ bins, call this discrete distribution
$Q$. We apply the same discretization to obtain $\mathbf{b} = (b_1, ..., b_m)$ from $\hat{\mathbf{e}}_Y$
and $\tilde{\mathbf{b}} = (\tilde{b}_1, ..., \tilde{b}_m)$ from $\hat{\mathbf{e}}_X$.

We then decide the causal direction as follows

\[ \begin{cases} 
    & X \rightarrow Y \quad \text{if} \quad C \leq W  \\
    & Y \rightarrow X \quad \text{if} \quad C > W  
 \end{cases}
\]

Where 

$$
    C = \norm{\mathbf{b} - \mathbf{u}}_{1} 
$$
$$
    W = \norm{\tilde{\mathbf{b}} - \mathbf{u}}_{1}
$$


s.t. $\mathbf{u} = (\frac{1}{m}, ..., \frac{1}{m})$.

Given our assumption about the \textbf{ANM}, the probability to output the correct causal direction is:

$$
   P_{\text{correct}} = \mathbb{P}\left(C \leq W\right) 
$$

We will assume that we have perfect regression estimates to simplify the proof; as we have seen in the 
previous chapter, with a little bit of work we can incorporate the error terms of the regression in 
the probability of correctness. 

We next upper bound this quantity in order to show consistency

\begin{align}
    \Prob\left(C \leq W\right) &\geq \Prob\left( \underset{\tau \in \mathbb{Q}}{\bigcup} C \leq \tau \cap W > \tau \right) \\
    &\geq \Prob\left(C \leq \tau \cap W > \tau \right) \\
    &\geq \Prob\left(C \leq \tau \right) - \Prob\left(W \leq \tau \right)
\end{align}

The first inequality is due to the fact that we are only taking the union in the rationals\footnote{We note that
we can only take unions over countable sets; recall also that the rationals are dense in the irrationals, so the
inequality is very close to equality (and in practice and among friends it would be).}. The second inequality is done by 
looking at the probability of a fixed $\tau$; and the final one follows by:

$$
    1 \geq \Prob\left( C \leq \tau \cup W > \tau \right) = 
    \Prob\left( C \leq \tau \right) + \Prob\left( W > \tau \right) - \Prob\left( C \leq \tau \cap W > \tau \right)
$$

We will next find appropriate bounds for $\Prob\left( C \leq \tau \right)$ and $\Prob\left(W \leq \tau \right)$.


We will first lower bound $\Prob\left( C \leq \tau \right)$ by upper bounding the complement event.

\begin{align}
    \Prob\left( C \geq \tau \right) &= \Prob\left( \sum_{i = 1}^{m} \abs{b_i - \frac{1}{m}} \geq \tau \right)  \\
    &\leq \Prob\left( m \operatorname{max}_{i} \abs{b_i - \frac{1}{m}} \geq \tau \right)  \\
    &= \Prob\left( \bigcup_{i} \; \abs{b_i - \frac{1}{m}} \geq \frac{\tau}{m} \right)  \\
    &\leq m \; \Prob\left( \abs{b_0 - \frac{1}{m}} \geq \frac{\tau}{m} \right) \\
    &\leq m2\exp \left( -2n \frac{\tau^2}{m^2} \right)
\end{align}

The second to last inequality follows by the union bound and by noting that all $b_i$s are the same since they
are discretized empirical distribution coming from a uniform source. For the final inequality we use Hoeffding's
inequality.


Recall that what is left to bound is the following quantity, $\Prob\left(W \leq \tau \right)$; 
for this we first define the following set of probability distributions:

$$
    \Gamma_\tau = \{ \pi \in  \Delta_m : \norm{\pi - U}_{L_1} \leq \tau \}
$$

Where the $\Delta_m$ is the $m$ dimensional simple and $U$ the uniform vector as before.

Observe that: 

$$
    \{ W \leq \tau \} = \{ \tilde{\mathbf{b}} \in \Gamma_\tau \}
$$

In essence, we are asking: "what is the chance that the realization of $\tilde{\mathbf{b}}$ --- which is the 
empirical distribution of some distribution $Q$ --- lies inside some set of distributions $\Gamma_\tau$.

We note that bounding this kind of event is exactly what Sanov's theorem\footnote{See the section on Information Theory and statistics in 
\cite{cover1999elements}} gives us, an important
result from large deviation theory that also exploits concentration of measure.



Let $\mathbf{x} = (x_1, ..., x_n)$ be a sequence of $n$ each drawn independently from 
a finite universe $U$ with $|U| = m$. Denote by $P_\mathbf{x}$ the empirical distribution --- 
or type --- for a given sequence $\mathbf{x}$. Let $Q^{n}$ be the product distribution $n$
independent samples of $Q$. 

\begin{theorem}[Sanov’s theorem]\label{sanov}

    Let $\Pi$ be a convex set of distributions on $U,$ and $m=|U| .$ Let

    \[
        P^{*}=\operatorname{argmin}_{P \in \Pi} D(P \| Q)
    \]
    
    Then
    \[
        \underset{Q^{n}}{\Prob}\left(P_{\mathbf{x}} \in \Pi\right) \leq(n+1)^{m} 2^{-nD\left(P^{*} \| Q\right)}
    \]
    
\end{theorem}

Applying the above theorem, and noting that $\Gamma_\tau$ takes the place of $\Pi$, $\tilde{\mathbf{b}}$ that of $P_{\mathbf{x}}$
and the discretized distribution $\hat{e}_X = X - \hat{f}_X(Y)$ that of $Q$ we get:

\begin{equation}
    \Prob\left(W \leq \tau \right) = \Prob\left( \tilde{\mathbf{b}} \in \Gamma_\tau \right) 
    \leq (n+1)^{m} 2^{-nD\left( \tau \right)}
\end{equation}

Where $D\left( \tau \right) := D\left(P^{*} \| Q\right)$, we make the $\tau$ relation explicit to 
keep in mind that the minimization is constrained to the set $\Gamma_\tau$ which depends on $\tau$.

We remark that the only place of concern is if $D\left(P^{*} \| Q\right) = 0$; assuming however that $Q \neq U$, then 
there will be some $\tau$ s.t. $Q \notin \Gamma_\tau$ and thus $D\left(P^{*} \| Q\right) \neq 0$.

We can now conclude by putting everything together; recall that we had shown that we could bound the success probability 
as follows:



\begin{align}
    \Prob\left(C \leq W\right) &\geq \Prob\left(C \leq \tau \right) - \Prob\left(W \leq \tau \right) \\
    &\geq 1 - 2m\exp \left( -2n \frac{\tau^2}{m^2} \right) - (n+1)^{m} 2^{-nD\left( \tau \right)}
\end{align}

This, if we fix $m$, and if there exists some $\tau$ s.t. $D\left( \tau \right) > 0$ then we get consistency
by letting $n \rightarrow \infty$.

We note that to get the best bound we may maximizes the right hand side w.r.t. $\tau$.

\subsection{Algorithm}

\begin{algorithm}[H]

    \caption{\textbf{Residual method}: Method to decide whether $P_{X, Y}$ satisfies and ANM $X \rightarrow Y$
        or $Y \rightarrow X$ , for an ANM given that the additive noise $P_Z$ is known.}
  
    \textbf{Input}:

    \begin{enumerate}
        \item I.i.d samples $\mathcal{D} = \{ (x_i, y_i )\}_{i \in [N]}$ of $X$ and $Y$
        \item Noise distribution $P_Z$
        \item Regression method
        \item Score estimator $\hat{C}: R^{m \times m} \rightarrow \R$
    \end{enumerate}
    
    \textbf{Output}: $\hat{C}_{X \rightarrow Y}$, $\hat{C}_{Y \rightarrow X}$, dir

    \begin{enumerate}

        \item \textbf{Estimate regressions} 
        
        \begin{itemize}
            \item[--] $\hat{f}_Y$ of the regression function $x \mapsto \E(Y | X=x)$
            \item[--] $\hat{f}_X$ of the regression function $y \mapsto \E(X | Y=y)$
        \end{itemize}

        \item \textbf{Estimate residuals} 
        
        \begin{itemize}
            \item[--] $ \mathbf{\hat{e}_{Y}} := \mathbf{y} - \hat{f}_Y(\mathbf{x})$
            \item[--] $ \mathbf{\hat{e}_{X}} := \mathbf{x} - \hat{f}_X(\mathbf{y})$
        \end{itemize}


        \item \textbf{Discrete distribution} 
       
        \begin{itemize}
            \item[--] $P^*_Z :=$ discretized distribution of $P_Z$
            \item[--] $b :=$ histogram of $ \mathbf{\hat{e}_{Y}}$
            \item[--] $\tilde{b} :=$ histogram of $ \mathbf{\hat{e}_{X}}$
        \end{itemize}

        \item \textbf{Compute scores} to measure the difference between the residuals
        \begin{itemize}
            \item[--] $\hat{C}_{X \rightarrow Y}: = \hat{C}( b, P^*_Z  )$ 
            \item[--] $\hat{C}_{Y \rightarrow X}: = \hat{C}( \tilde{b}, P^*_Z  )$
        \end{itemize}        

        \item Output $\hat{C}_{X \rightarrow Y}$, $\hat{C}_{Y \rightarrow X}$, and
        
        \[ 
        \text{dir} :=  
         \begin{cases} 
            & X \rightarrow Y \quad \text{if} \; \hat{C}_{X \rightarrow Y} \leq \hat{C}_{Y \rightarrow X}\\
            & Y \rightarrow X \quad \text{otherwise}
         \end{cases}
        \]
        
    \end{enumerate}

  \label{alg:residual}
  \end{algorithm}