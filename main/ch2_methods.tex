\chapter{Causal Inference}

\section{Bivariate causal model}

\subsection{ANM}

We will now introduce\footnote{Note that this introduction follows closely that of \cite{Mooij2016jmlr}, and 
we encourage the reader to have a look to fill in details that have been omitted here.} 
the bivariate causal model and define the particular subset of such models that 
we will work on. For the bivariate case, if we have $Y \in \R$ as a direct cause of $X \in \R$, then we can model 
the relationship as follows:

\begin{equation}
    \begin{cases} 
        & Y = f(X, Z) \\
        & X \bigCI Z,\quad X \thicksim P_X,\quad Z \thicksim P_{Z}  
     \end{cases}
     \label{causal_model}
\end{equation}

where $P_X$ is the density of the cause and $P_Z$ that of the latent variable; $f: \R \times \R \rightarrow \R$
is a borel measurable function (w.r.t. to the Borel sets of $\R \times \R$ and $\R$). 

If we assume that there is no confounder, no sampling bias\footnote{If you consider the 
example given in the introduction, then if Bob always sampled when Alice was exercising, then this 
would have lead to sampling bias, as during exercise both heart rate and blood pressure tend to increase.} 
and no cycles then it is natural to assume that $X \bigCI Z$.

A confounder is a third variable say $Z$, that influences both $X$ and $Y$; for example in figure \ref{fig:conditionalIndep}
, if the edges are directed from $X$ to $Z$ and also from $Z$ to $Y$, then we would say that $Z$ is a confounder.

While we can model $Z$ as either a scalar or a vector, we can without loss of generality assume it to be a scalar.
It can be shown that if $Z$ is a vector, then one can construct a simpler model with scalar noise, which has the 
same observational and interventional distribution \cite{Mooij2016jmlr}.

An important remark is that given the direct model in equation \ref{causal_model}, we can find some 
$\tilde{f}$ and $\tilde{Z}$ such that

\begin{equation}
    \begin{cases} 
        & X = \tilde{f}(Y, \tilde{Z}) \\
        & Y \bigCI \tilde{Z},\quad Y \thicksim P_Y,\quad \tilde{Z} \thicksim P_{\tilde{Z}}  
     \end{cases}
\end{equation}

with the important property that it induces an equivalent observational distribution $p_{X, Y}$ as that of 
equation \ref{causal_model}. However in general the interventional distribution will differ. In particular 
this means that with observational data alone we are not able identify the right causal direction.

We must therefore make further assumptions on $f$ that break this symmetry and allow us to make causal inference
using observational data alone.
In particular we will consider the following class of models. Note that these models are a subset of those 
that we just introduced (equation \ref{causal_model}).

\begin{definition} Given a triplet $(P_X, P_Z, f)$, consisting of two finite mean densities and a Borel-measurable function
$f: \R \rightarrow \R$, we can define a \textbf{bivariate Additive Noise Model (ANM)} $X \rightarrow Y$
\[
    \begin{cases} 
        & Y = f(X) + Z \\
        & X \bigCI Z,\quad X \thicksim P_X,\quad Z \thicksim P_{Z}  
     \end{cases}  
\]
If the induced joint density of $X$ and $Y$, $P_{X, Y}$ has a density with respect to Lebesgue measure, 
we say that $P_{X, Y}$ satisfies the ANM $X \rightarrow Y$.
\end{definition}

Given such a model, we are interested in the cases when the observational distribution $P_{X, Y}$ can only
lead to only one causal explanation; this motivates the following definition:

\begin{definition} If the joint density $P_{X, Y}$ satisfies an ANM $X \rightarrow Y$, but does not satisfy
ANM $Y \rightarrow X$, then we call the ANM $X \rightarrow Y$ \textbf{identifiable}. 
\end{definition}

Intuitively, non-linearities due to $f$ will break the symmetry needed to make a reverse ANM. This is what
\cite{hoyer2009nonlinear} explore; they show that for the 
a triplet $(P_X, P_Z, f)$ to generate a non-identifiable ANM, it needs to satisfy a particular 
differential equation. Loosely speaking this cannot happen in the generic case: in other words the forward 
model $X \rightarrow Y$ cannot be inverted. 



\newpage
If $f$ is linear, then one can give a much more precise statement about identifiability:

\begin{theorem} Let $X$ and $Y$ be random variables, such that
$$
    Y = aX + Z, \quad X \bigCI Z, \; a \neq 0
$$
Then we can reverse the process, i.e. there exists $\tilde{a} \in \R$ and a noise $\tilde{Z}$ such that
$$
    X = \tilde{a}Y + \tilde{Z}, \quad Y \bigCI \tilde{Z}
$$
if and only if $X, Y, Z, \tilde{Z}$ are Gaussian distributed. 
\label{linear_indentifiability}
\end{theorem}

The proof is a simple application of the Darmois-Skitovich Theorem\footnote{ The theorem also plays an important role
in independent component analysis (ICA), in short it deals with source separation. It turns out that if we have the 
linear multivariate causal setting, then we can cast it as a source separation problem and use ICA to solve it. 
Note that there too Gaussianty makes or breaks the method.} (\cite{darmois}); which we state without proof:

\begin{theorem}[Darmois-Skitovich] Let $X_i$, $i \in [n]$ be independent random variables, and let
    $\alpha_i$, $\beta_j$ be non zero constants. Then, if the random variables
    $$ 
        L_1 = \sum_{i \in [n]} \alpha_i X_i
    $$
    $$ 
        L_2 = \sum_{i \in [n]} \beta_i X_i
    $$
    are independent, i.e. $L_1 \bigCI L_2$; then all the random variables $X_i$ are Gaussian. 
\end{theorem}

We now prove Theorem \ref{linear_indentifiability} using the Darmois-Skitovich Theorem.

\begin{proof} 
    ~
For the "only if" part, note that by simple manipulation, we have the following:

\[
    \begin{bmatrix}
        Y \\
        \tilde{Z}
    \end{bmatrix}
    =
    \begin{bmatrix}
    a & 1 \\
    1 - \tilde{a}a & -\tilde{a}
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        X \\
        Z
    \end{bmatrix}
\]

If $\tilde{a} \neq 0$ and $1 - \tilde{a}a \neq 0$ then by Darmois-Skitovich, the result follows.

We next show that both of these conditions must be true for the process to be reversible:

\begin{enumerate}
    \item If $\tilde{a} = 0$ then $\tilde{Z} = X$, but then $X \bigCI Y$, a contradiction\footnote{
        Note that $X \bigCI aX + Z$ is trivially false 
        in the discrete case, but if both are continuous then we need to be a bit more careful as is the 
        case with degenerate random variables; but essentially the same holds (see \cite{Peters2008diploma}).
    }.
    \item Finally, if $1 - \tilde{a}a = 0$ then $\tilde{Z} = -\tilde{a}Z$, and thus 
    $-\tilde{a}Z \bigCI Y$, a contradiction.
\end{enumerate}

We have thus show the "only if" part. 

We next show the "if" part; first assume that $X$ and $Z$ are Gaussian random variables. 
It is easy to verify that:
$\operatorname{Cov}(Y, \tilde{Z}) = a(1 - a\tilde{a})\operatorname{Var}(X) - \tilde{a}\operatorname{Var}(Z)$.
Thus if we set $\tilde{a} = \frac{a \operatorname{Var}(X)}{a^2 \operatorname{Var}(X) + \operatorname{Var}(Z)}$ 
we get that $\operatorname{Cov}(Y, \tilde{Z}) = 0$, and since they are Gaussian random variables we get 
that they are also independent.

\end{proof}

These results show that indeed, in most cases, we should be able to perform causal inference as most 
additive models should be identifiable. Interestingly, the Gaussian setting provides difficulty  due 
to the symmetry of the Gaussian (in the linear case). In general the Gaussian is our friend, but not today.
We will next explore some methods for causal inference.

\section{ANM Methods}

Additive noise model (ANM) methods are a very simple score based family of methods for causal inference.
The following lemma\footnote{A simple proof can be found here \cite{Mooij2016jmlr}} motivates the method:

\begin{lemma}
    Given a joint density $P_{X, Y}$ of two random variables $X$, $Y$ s.t. the conditional expectation
    $\E(Y |X = x)$ is well-defined for all x and measurable. Then, $P_{X, Y}$ satisfies a bivariate 
    Additive Noise Model $X \rightarrow Y$ if and only if $E_Y := Y - \E(Y|X)$ has 
    finite mean and is independent of $X$.
\end{lemma}

In practice we get some data from $P_{X, Y}$ ; say 
$\mathcal{D}_N = \{ (x_i, y_i )\}_{i \in [N]}$. We can then either split it into a test/train in order to 
first fit a regression which we then evaluate using the test set. If the data is scarce we may alternatively 
recycle the data --- i.e. reuse it for both training and evaluation.

First we estimate through regression the function $x \mapsto \E(Y | X=x)$, say $\hat{f}$; we 
then compute the estimated residual $\hat{e} = \hat{f}(X) - Y$. 
Next, we estimate the dependence between $\hat{e}$ and $X$ using a score function $C$; e.g. $C$ could be 
the empirical mutual information between them. Thus a low score would be evidence for an ANM in that direction; 
we can compute the score for the reverse model by switching the roles of $X$ and $Y$. We can then compare 
the scores and use this as a criteria for inference. We write down this idea more explicitly 
in Algorithm \ref{alg:anm_general}.

In order to show that such a procedure is consistent\footnote{By consistent we mean that the probability of
predicting the true causal direction goes to 1 as the number of samples $n \rightarrow \infty$.
} we need 3 things:

\begin{enumerate}
    \item $P_{X, Y}$ satisfies either $X \rightarrow Y$ or $Y \rightarrow X$, but not both.
    \item The regression method should be \textbf{suitable} for regressing $Y$ on $X$.
    \item If $X \rightarrow Y$, then asymptotically $\hat{C}_{X \rightarrow Y} < \hat{C}_{Y \rightarrow X}$
\end{enumerate}


We take point 1 as an assumption as there is currently no theoretical result that allows for a consistent 
test to check if $P_{X, Y}$ satisfies an ANM $X \rightarrow Y$. 

In point 2, we will say that a regression is \textbf{suitable} if the following holds:
Given two real-valued random variable $X$, $Y$, with joint distribution $P_{X, Y}$. If 
we are given two sequences --- for training and test --- say $D_N = X_1, ..., X_N$ and 
$D_N^\prime = X^\prime_1, ..., X^\prime_N$. We say that a regression method is \textbf{suitable} for 
regressing $Y$ on $X$ satisfies

$$
\lim _{N \rightarrow \infty} \frac{1}{N} 
\sum_{n=1}^{N} \E_{\mathcal{D}_{N}, \mathcal{D}_{N}^{\prime}}\left(\left|\hat{f}_{Y}\left(X_{n}^{\prime} ; 
\mathcal{D}_{N}\right)-\E\left(Y \mid X=X_{n}^{\prime}\right)\right|^{2}\right)=0
$$



\begin{algorithm}%[H]%[tbp]

    \caption{General procedure to decide whether $P_{X, Y}$ satisfies and ANM $X \rightarrow Y$
        or $Y \rightarrow X$}
  
    \textbf{Input}:

    \begin{enumerate}
        \item i.i.d samples $\mathcal{D}_N = \{ (x_i, y_i )\}_{i \in [N]}$ of $X$ and $Y$
        \item Regression method
        \item Score estimator $\hat{C}: \R^n \times \R^n \rightarrow \R$
    \end{enumerate}
    
    \textbf{Output}: $\hat{C}_{X \rightarrow Y}$, $\hat{C}_{Y \rightarrow X}$, dir

    \begin{enumerate}

        \item Split the $\mathcal{D}_N$ in half randomly to obtain  $\mathcal{D}_{train}$ 
        and $\mathcal{D}_{test}$

        \item Use the regression method on the training data $\mathcal{D}_{train}$:
        \begin{itemize}
            \item[--] $\hat{f}_X$ of the regression function $x \mapsto \E(Y | X=x)$
            \item[--] $\hat{f}_Y$ of the regression function $y \mapsto \E(X | Y=y)$
        \end{itemize}

        \item Estimate residuals using the predicted regressions on the test data $\mathcal{D}_{test}$:
        \begin{itemize}
            \item[--] $ \mathbf{\hat{e}_Y} := \mathbf{y} - \hat{f}_Y(\mathbf{x})$
            \item[--] $ \mathbf{\hat{e}_X} := \mathbf{x} - \hat{f}_X(\mathbf{y})$
        \end{itemize}

        \item Compute scores to measure dependence between inputs and estimated residuals based on the
        test data $\mathcal{D}_{test}$
        \begin{itemize}
            \item[--] $\hat{C}_{X \rightarrow Y}: = \hat{C}(\mathbf{x}, \mathbf{\hat{e}_Y})$ 
            \item[--] $\hat{C}_{Y \rightarrow X}: = \hat{C}(\mathbf{y}, \mathbf{\hat{e}_X})$
        \end{itemize}        

        \item Output $\hat{C}_{X \rightarrow Y}$, $\hat{C}_{Y \rightarrow X}$, and
        
        \[ 
        \text{dir} :=  
         \begin{cases} 
            & X \rightarrow Y \quad \text{if} \; \hat{C}_{X \rightarrow Y} \leq \hat{C}_{Y \rightarrow X}\\
            & Y \rightarrow X \quad \text{otherwise}
         \end{cases}
        \]
        
    \end{enumerate}

  \label{alg:anm_general}
  \end{algorithm}


  We will now overview some score functions, note that the list is far from exhaustive and other methods can be found 
  in \cite{Mooij2016jmlr}. We end the chapter by presenting some alternative methods. 

\subsection{HSIC score}

First considered by \cite{hoyer2009nonlinear}, is the Hilbert-Schmidt independence Criterion (HSIC) for
testing the independence between the residuals and the input. 

$$
  \hat{C}(\mathbf{u}, \mathbf{v}) := \widehat{\operatorname{HSIC}}_{k, l} (\mathbf{u}, \mathbf{v})
$$

We will give a formal description of the HSIC in the next chapter, where we will develop some of the background 
to get an intuition on this measure. For now, one can think of the HSIC as a metric that computes the distance 
between the the product distribution and the joint (similar to the Mutual information); the key difference is that 
we provide two kernels, $k$ and $l$ which will transform the distributions to a different space. For specific 
kernel choices the HSIC indeed becomes a metric. 

Note that one can approximate the distribution of $\widehat{\operatorname{HSIC}}$, and use hypothesis testing 
for the independence test. 

\subsection{Entropy score}

Another type of score function looks at differential entropies instead of directly testing for independence.
These ideas stem from \cite{kpotufe2014consistency} and \cite{nowzohour2016score}; The following lemma shows how this might be used in practice:

\begin{lemma} Consider random variables $X$ and $Y$, with joint density $P_{X, Y}$. For any functions 
$f, g : \R \rightarrow \R$ we have:

$$
    H(X) + H(Y - f(X)) = H(Y) + H(X - g(X)) - I(X - g(X), Y) + I(Y - f(X), X)
$$

where $H(\cdot)$ denotes the differential entropy and $I(\cdot, \cdot)$ denotes the differential mutual information 
(\cite{cover1999elements}).
\end{lemma}

The proof is a simple application of the chain rule. Note that if $X \rightarrow Y$ then $I(Y - f(X), X) = 0$; 
since $I(X, Y) \geq 0$ for any $X$, $Y$ it follows
that:

$$
    H(X) + H(Y - f(X)) \leq  H(Y) + H(X - g(X))
$$

Which motivates the score function

$$
    C(U, V) = H(U) + H(V)
$$

This approach to estimate the direction of the ANM is consistent under certain assumptions as is shown by 
\cite{kpotufe2014consistency} and \cite{nowzohour2016score}. One of the main drawbacks of using differential
entropy is that we need to go through discretization, which can lead to undesired effects --- and can also be 
inefficient in higher dimensions. 


\subsection{CGNN: Causal Generative Neural Nets}

In the work of \cite{goudet2017causal}, the authors estimate a \textit{generative} model by approximating
the FCM structure using neural networks given some data from $P_{X, Y}$. Using the same notation
as in the introduction on FCM, the idea is to estimate each $f_i$ by a neural network, and the 
search through the DAG space. Since the DAG space is super-exponential in the number of variables,
they apply a greedy procedure to decide whether or not to include an edge $X_i \rightarrow X_j$.
In some sense this is similar to greedy methods used in model selection. The caveat here is that
using the generative model they can backpropagate to learn all the $f_i$ simultaneously. 

More specifically, given the current graph estimate $\mathcal{G}$, they can generate some $\hat{P}$
from the current $f_i$ and some noise $\mathcal{E}$. Then they can train the model by using 
the MMD\footnote{We present and give some background on the MMD in the next chapter} 
as a loss function between $\hat{P}$ and $P$ (the data distribution).

So far the CGNN appears to have the best performance on the various benchmarks. 

\subsection{Other methods}



\textbf{IGIC} (Information Geometric Causal Inference), the method stems from \cite{janzing2010causal}.  
The idea is based on the following observation: If $X \rightarrow Y$, then a reasonable assumption is that 
$P_X$ contains no information about $P_{Y | X}$. If one thinks about $X$ as as the distribution of 
elevations in a country, and $Y$ the temperature, then this assumption is likely to hold. The difficulty 
lies in formalizing the notion of "information" in a good way. 

There are many other methods some of which can be found in the review of \cite{Mooij2016jmlr}.
