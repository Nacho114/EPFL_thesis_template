\chapter{Causal Inference}

\section{Bivariate causal model}

\subsection{ANM}

We will now introduce\footnote{Note that this introduction follows closely that of \cite{Mooij2016jmlr}, and 
we encourage the reader to have a look to fill in details that have been omitted here.} 
the bivariate causal model and define the particular class of such models that 
we will work on. To avoid any sort of technical complications, hen we talk about functions, 
they will always be Borel measurable -- which in any case consists of most functions of interest.

For the bivariate case, if we have $Y \in \R$ as a direct cause of $X \in \R$, then we can model 
the relationship as follows:

\begin{equation}
    \begin{cases} 
        & Y = f(X, Z) \\
        & X \bigCI Z,\quad X \thicksim p_x,\quad Z \thicksim p_{Z}  
     \end{cases}
     \label{causal_model}
\end{equation}

If we assume that there is no confounder, that there is no sampling bias\footnote{If you consider the 
example given in the introduction, then if Bob always sampled when Alice was excercising, then this 
would have lead to sampling bias, as during excercise both heart rate and blood pressure tend to increase.} 
and no cycles then it is natural to assume that $X \bigCI Z$.

While we can model $Z$ as either a scalar of a vector, we can without loss of generality assume it to be a scalar.
It can be shown that if $Z$ is a vector, then one can construct a simpler model with scalar noise, which has the 
same observational and interventional distribution \cite{Mooij2016jmlr}.

An important remark is that given the direct model in equation \ref{causal_model}, we can find some 
$\tilde{f}$ and $\tilde{Z}$ such that

\begin{equation}
    \begin{cases} 
        & X = \tilde{f}(Y, \tilde{Z}) \\
        & Y \bigCI \tilde{Z},\quad Y \thicksim p_y,\quad \tilde{Z} \thicksim p_{\tilde{Z}}  
     \end{cases}
\end{equation}

with the important property that it induces an equivalent observational distribution $p_{X, Y}$ as that of 
equation \ref{causal_model}. However in general the interventional distribution will differ. In particular 
this means that with observational data alone we are not able identify the right causal direction.

We must therefore make further assumptions on $f$ that break this symmetry and allow us to make causal inference.
In particular we will consider the following subset of models from the ones introduced in equation \ref{causal_model};
where we restric the noise to be additive.

\begin{definition} Given a triplet $(p_X, p_Z, f)$, consisting of two finite mean densities and a function
$f: \R \rightarrow \R$, we can define a \textbf{bivariate Additive Noise Model (ANM)} $X \rightarrow Y$
\[
    \begin{cases} 
        & Y = f(X) + Z \\
        & X \bigCI Z,\quad X \thicksim p_X,\quad Z \thicksim p_{Z}  
     \end{cases}  
\]
If the induced density $p_{x, y}$ has a density with respect to Lebesgue measure, we say that $p_{x, y}$
satisfies the ANM $X \rightarrow Y$.
\end{definition}

Given such a model, we are interested in the cases when the observational distribution $p_{x, y}$ can only
lead to one causal explenation; this motivates the following definition:

\begin{definition} If the joint density $p_{x, y}$ satisfies an ANM $X \rightarrow Y$, but does not satisfy
ANM $Y \rightarrow X$, then we call the ANM $X \rightarrow Y$ \textbf{identifiable}. 
\end{definition}

Intuitively non-linearities due to $f$ will break the symmetry needed to find a reverse ANM. This is what
\cite{hoyer2009nonlinear} and friends explore; they show that for the 
a triplet $(p_X, p_Z, f)$ to generate a non-identifiable can happen only if they satisfy a particular 
differential equation that loosely speaking cannot happen in the generic case: in other words the forward 
model $X \rightarrow Y$ cannot be inverted. 

If $f$ is linear, then one can give a much more precise statement about Identifiability:

\begin{theorem} Let $X$ and $Y$ be random variables, such that
$$
    Y = aX + Z, \quad X \bigCI Z, \; a \neq 0
$$
Then we can reverse the process, i.e. there exists $\tilde{a} \in \R$ and a noise $\tilde{Z}$ such that
$$
    X = \tilde{a}Y + \tilde{Z}, \quad Y \bigCI \tilde{Z}
$$
if and only if $X, Y, Z, \tilde{Z}$ are Gaussian distributed. 
\label{linear_indentifiability}
\end{theorem}

The proof is a simple appilcation of the Darmois-Skitovich Theorem. The theorem also plays an imporant role
in ICA; it states the following:

\begin{theorem}[Darmois-Skitovich] Let $X_i$, $i \in [n]$ be independent random variables, and let
    $\alpha_i$, $\beta_j$ be non zero constants. Then, if the random variables
    $$ 
        L_1 = \sum_{i \in [n]} \alpha_i X_i
    $$
    $$ 
        L_2 = \sum_{i \in [n]} \beta_i X_i
    $$
    are independent, i.e. $L_1 \bigCI L_2$; then all the random variables $X_i$ are gaussian. 
\end{theorem}

We now prove Theorem \ref{linear_indentifiability} using the Darmois-Skitovich Theorem.

\begin{proof} 
    ~
For the "only if" part, note that by simple manipulation, we have the following:

\[
    \begin{bmatrix}
        Y \\
        \tilde{Z}
    \end{bmatrix}
    =
    \begin{bmatrix}
    a & 1 \\
    1 - \tilde{a}a & -\tilde{a}
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        X \\
        Z
    \end{bmatrix}
\]

If $\tilde{a} \neq 0$ and $1 - \tilde{a}a \neq 0$ then by Darmois-Skitovich, the result follows.

We will next show that both of these conditions must be true for the process to be reversible:

\begin{enumerate}
    \item If $\tilde{a} = 0$ then $\tilde{Z} = X$, but then $X \bigCI Y$, a contradiction\footnote{
        Note that $X \bigCI aX + Z$ is trivially false 
        in the discrete case, but if both are continuous then we need to be a bit more careful as is the 
        case with degenerate random variables; but essentially the same holds (see \cite{Peters2008diploma}).
    }.
    \item Finally, if $1 - \tilde{a}a \neq 0$ then $\tilde{Z} = -\tilde{a}Z$, and thus 
    $-\tilde{a}Z \bigCI Y$, a contradiction.
\end{enumerate}

We have thus show the "only if" part. 

We next show the "if" part; first assume that $X$ and $Z$ are Gaussian random variables. 
It is easy to verify that:
$\operatorname{Cov}(Y, \tilde{Z}) = a(1 - a\tilde{a})\operatorname{Var}(X) - \tilde{a}\operatorname{Var}(Z)$.
Thus if we set $\tilde{a} = \frac{a \operatorname{Var}(X)}{a^2 \operatorname{Var}(X) + \operatorname{Var}(Z)}$ 
we get that $\operatorname{Cov}(Y, \tilde{Z}) = 0$, and since they are gaussian random variables we get 
that they are also independent.


\end{proof}


\section{Methods}

\subsection{Introduction}

The following lemma\footnote{A simple proof can be found here \cite{Mooij2016jmlr}} motivates the 
first class of methods that we will present:

\begin{lemma}
    Given a joint density $p_{x, y}$ of two random variables $X$, $Y$ s.t. the conditional expectation
    $\E(Y |X = x)$ is well-defined for all x and measurable. Then, $p_{x, y}$ satisfies a bivariate 
    Additive Noise Model $X \rightarrow Y$ if and only if $E_Y := Y - \E(Y|X)$ has 
    finite mean and is independent of $X$.
\end{lemma}

In practice we get some data from $p_{x, y}$ ; say 
$\mathcal{D}_N = \{ (x_i, y_i )\}_{i \in [N]}$. We can then either split it into a test/train in order to 
first fit a regression which we then evaluate using the test set. If the data is scarce we may alternatively 
recycle the data -- i.e. reuse it for both training and evaluation.

First we estimate through regression the function $x \mapsto \E(Y | X=x)$, say $\hat{f}$; we 
then compute the estimated residual $\hat{e} = \hat{f}(X) - Y$. 
Next, we estimate the dependence between $\hat{e}$ and $X$ using a score function $C$; i.e. $C$ could be 
the empirical mutural information betheen them. Thus a low score would be evidance for an ANM in that direction; 
we can compute the score for the reverse model by switching the roles of $X$ and $Y$. We can then compare 
the scores and use this as a criteria for inference. We write down this idea more explictly 
here Algorithm \ref{alg:anm_general}.

In order to show that such a procedure is consistent we need 3 things:

\begin{enumerate}
    \item $p_{x, y}$ satisfies either $X \rightarrow Y$ or $Y \rightarrow X$, but not both.
    \item The regression method should be \textbf{suitable} for regressing $Y$ on $X$.
    \item If $X \rightarrow Y$, then asymptotically $\hat{C}_{X \rightarrow Y} < \hat{C}_{Y \rightarrow X}$
\end{enumerate}


We will take point 1 as an assumption as there is currently no theoretical result that allows for a consistent 
test to check if $p_{x. y}$ satisfies an ANM $X \rightarrow Y$. 

In point 2, by we \textbf{suitable} regression we mean that $\norm{\mathbf{\hat{e}} - \mathbf{e}} \rightarrow 0$.
Essentially we require the regression used on the data to have 0 mean square error in expectation\footnote{A 
more precise statement can be found in \cite{Mooij2016jmlr}}. 

\begin{algorithm}[H]%[tbp]

    \caption{General procedure to decide whether $p_{x, y}$ satisfies and ANM $X \rightarrow Y$
        or $Y \rightarrow X$}
  
    \textbf{Input}:

    \begin{enumerate}
        \item I.i.d samples $\mathcal{D}_N = \{ (x_i, y_i )\}_{i \in [N]}$ of $X$ and $Y$
        \item Regression method
        \item Score estimator $\hat{C}: \R^n \times \R^n \rightarrow \R$
    \end{enumerate}
    
    \textbf{Output}: $\hat{C}_{X \rightarrow Y}$, $\hat{C}_{Y \rightarrow X}$, dir

    \begin{enumerate}

        \item Split the $\mathcal{D}_N$ in half randomly to obtain  $\mathcal{D}_{train}$ 
        and $\mathcal{D}_{test}$

        \item Use the regression method on the training data $\mathcal{D}_{train}$:
        \begin{itemize}
            \item[--] $\hat{f}_X$ of the regression function $x \mapsto \E(Y | X=x)$
            \item[--] $\hat{f}_Y$ of the regression function $y \mapsto \E(X | Y=y)$
        \end{itemize}

        \item Estimate residuals using the predicted regressions on the test data $\mathcal{D}_{test}$:
        \begin{itemize}
            \item[--] $ \mathbf{\hat{e}_Y} := \mathbf{y} - \hat{f}_Y(\mathbf{x})$
            \item[--] $ \mathbf{\hat{e}_X} := \mathbf{x} - \hat{f}_X(\mathbf{y})$
        \end{itemize}

        \item Compute scores to measure dependence between inputs and estimated residuals based on the
        test data $\mathcal{D}_{test}$
        \begin{itemize}
            \item[--] $\hat{C}_{X \rightarrow Y}: = \hat{C}(\mathbf{x}, \mathbf{\hat{e}_Y})$ 
            \item[--] $\hat{C}_{Y \rightarrow X}: = \hat{C}(\mathbf{y}, \mathbf{\hat{e}_X})$
        \end{itemize}        

        \item Output $\hat{C}_{X \rightarrow Y}$, $\hat{C}_{Y \rightarrow X}$, and
        
        \[ 
        \text{dir} :=  
         \begin{cases} 
            & X \rightarrow Y \quad \text{if} \; \hat{C}_{X \rightarrow Y} \leq \hat{C}_{Y \rightarrow X}\\
            & X \rightarrow Y \quad \text{otherwise}
         \end{cases}
        \]
        
    \end{enumerate}

  \label{alg:anm_general}
  \end{algorithm}


\subsection{HSIC}

First considered by \cite{hoyer2009nonlinear}, is the Hilbert-Schmidt independence Criterion (HSIC) for
testing the independence between the residuals and the inpurts. 

definition of HSIC here

By choosing an appropriate kernel, the HSIC becomes a metric, and so theoretically it is gauranteed to 
be 0 iff $X \bigCI Y$. We note that in Chapter 3 we give an overview of the reproducing kernel hilbert 
space which will allow us to get a better intuition for the HSIC. 

\subsection{Entropy based}

Another type of score function looks at differential entropies instead of directly testing for independence.
These ideas stem from A and B; The following lemma shows how this might be used in practice:

\begin{lemma} Consider random variables $X$ and $Y$, with joint density $p_{x, y}$. For any functions 
$f, g : \R \rightarrow \R$ we have:

$$
    H(X) + H(Y - f(X)) = H(Y) + H(X - g(X)) - I(X - g(X), Y) + I(Y - f(X), X)
$$

where $H(.)$ denotes the differential entropy and $I(.,.)$ denotes the differential mutual information 
(\cite{cover1999elements}).
\end{lemma}

The proof is a simple application of the chain rule. Note that if $X \rightarrow Y$ then $I(Y - f(X), X) = 0$; 
since $I(X, Y) \geq 0$ for any $X$, $Y$ it follows
that:

$$
    H(X) + H(Y - f(X)) \leq  H(Y) + H(X - g(X))
$$

Which motivates the score function

$$
    C(U, V) = H(U) + H(V)
$$

This approach to estimate the direction of the ANM is consistent under certain assumptions as is shown by 
\cite{kpotufe2014consistency} and \cite{nowzohour2016score}. One of the main drawbacks of using differential
entropy is that we need to go through discretization, which can lead to undesired effects.

\subsection{CGNN: Causal Generative NNets}

In the work of \cite{goudet2017causal}, they estimate a \textit{generative} model by approximating
the FCM structure using neural networks given some data from $p_{x, y}$. Using the same notation
as in the introduction on FCM, the idea is to estimate each $f_i$ by a neural network, and the 
search through the DAG space. Since the DAG space is super exponetial in the number of variables,
they apply a greedy procedure to decide whether or not to include an edge $X_i \rightarrow X_j$.
In some sense this is similar to greedy methods used in model selection. The caveat here is that
using the generative model they can backpropagate to learn all the $f_i$ simulatneously. 

More specifically, given the current graph estimate $\mathcal{G}$, they can generate some $\hat{P}$
from the current $f_i$ and some noise $\mathcal{E}$. Then they can train the model by using 
the MMD\footnote{We present and give some background on the MMD in the next chapter} 
as a loss function between $\hat{P}$ and $P$ (the data distribution).

So far the CGNN appears to have the best performance on the various benchmarks. 

\subsection{Other methods}

There are other methods such as the  ICGC


\section{SNR and causality}

In this section we give a brief commentary about causality from the perspective of communication theory.

In virtually most of the predictive fields, noise is the enemey; Indeed, in the absence of noise, finding the
best linear fit to a linear model is trivial. Emre Telatar, a powerful information theorist, liked to jest
in his digital communication course that "without noise, we communication engineers would be without a job".

Indeed, for most of the early 20th century\footnote{quote comm book} noise was keeping engineers busy as they 
devised clever schemes to fight noise. At the time the whole business was very experimental as
no one had come close to understanding noise in the context of transmission; questions such 
as "Is it possible to send a message with arbitrary reliability?" and "What is the theoretical maximum amount
of information that can be reliably sent?", were questions that no one had come close to solving.

Then Shannon came along, in his Magnum Opus, \cite{shannon}, he not only formalised the foundations of information
Theory, but he also proved\footnote{Shannon had a very deep...} most of the main results in it. In particular, he
showed that for the AWGN (Additive white nois gaussian channel), it is possible to realiably transmit at most $C$
bits per time unit:

$$
C \propto log(1 + SNR)
$$

Where $SNR$ is the celebrated signal to noise ratio -- $SNR = \frac{\E(X^2)}{\E(N^2)}$. As we 
would expect, if $SNR \rightarrow \infty$ then we can send an arbitrary amount of information 
per time unit (the only limit is the physical one, i.e. the speed of light). Conversely if $SNR = 0$
then we find ourseleve at a rave, not matter how much we yell, our friends will not be able to 
understand us.

If we return to the question of causality; a somehwhat trivial observation is that if the mechanism 
is injective\footnote{If it is not injective, then the function is not invertible, and thus only one 
causal direction is possible.}, in the absence of noise, it is 
not possible to say anything about the causal direction of the mechanism. Here too noise is the
benelovent giver of jobs, albeit not for the same reasons. Interestingly, we can use noise to 
help us deduce the causal nature of a process. 

We will now see what perhaps could be considered the most simple causal set up, and describe a method
for causal inference. We will then see that $SNR$ also plays an important role.

Consider the linear additive noise model -- our first causal model!

\[ \begin{cases} 
    & Y = aX + E_Y  \\
    & X \bigCI E_Y,\; X \thicksim p_x,\; E_Y \thicksim p_{E_Y}  
 \end{cases}
\]

Suppose we are given $n$ samples of the above process:

$$
    y_i = ax_i + z_i, i \in \left[n\right]
$$

We collect these into vectors say $y$, $x$ and $z$; note that we do not have
access to the latter, but it will come in handy for the derivation that follows.

One common idea is to first compute the residuals for both possible regression models,
and the to check which residual is less dependent on $x$ and $y$ respectively -- we are
testing for the noise / data independence hypothesis.

We first regress $y$ on $x$. i.e.

$$
    \hat{a} = \operatorname{argmax}_{\alpha} \norm{y - \alpha x}_{2}^2
$$

We differentiate w.r.t to $\alpha$:

$$
    -2y^\intercal x + 2\alpha \norm{x}_2^2 = 0 \qquad \Rightarrow \qquad 
    \alpha = \frac{y^\intercal x}{\norm{x}_2^2} = 
    \frac{a\norm{x}_2^2 + z^\intercal x}{\norm{x}_2^2}
$$

Note that by symmetry, we find that if we regress x on y we get

$$
    \tilde{a} = \frac{x^\intercal y}{\norm{y}_2^2} = 
    \frac{a\norm{x}_2^2 + z^\intercal x}{a^2 \norm{x}_2^2 + 2a x^\intercal z + \norm{z}^2_2}
$$

As $n \rightarrow \infty$ we can invoke the Law of Large Numbers\footnote{The samples are iid, 
and we note that convergance in probability is preserved when taking products and 
continuous mappings.} and we thus -- given that $E(z) = \E(N) = 0$ -- find:

$$
    \E(\hat{a}) = \frac{a\E(\norm{x}_2^2) + 0}{\E(\norm{x}_2^2)} \xrightarrow{p} a 
$$

$$
    \E(\tilde{a}) = \frac{a\E(\norm{x}_2^2) + 0}{
        a^2 \E(\norm{x}_2^2) + 0 + \E(\norm{z}^2_2)} \xrightarrow{p}
        \frac{a SNR}{a^2 SNR + 1}
$$

Thus for large $n$ we have that:

$$
    r_{x \rightarrow y} \approx y - ax = z
$$

and

$$
    r_{y \rightarrow x} = x - \tilde{a}y \approx x - \frac{a SNR}{a^2 SNR + 1} (ax + z) 
$$

Observe that if $SNR = 0$, then $r_{y \rightarrow x} \approx x$, in which case whaat??

If however $SNR \rightarrow \infty$, then $r_{y \rightarrow x} \approx -\frac{1}{a}z \approx \frac{1}{a}z$, thus 
the residuals carry no information about causality.

We note that this somewhat formalises the intuition that we had about the role of noise in 
causality; it also shows that indeed, $SNR$ plays an inverted role vis-a-vis that of communication
theory. 

If we are interested in finite sample results if we demand a certain accuracy for a given
sample size, then we conjecture that $SNR$ will play a key role in determining this.
