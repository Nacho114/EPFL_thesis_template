\chapter{Conclusion}

Good job

\section{TODO}

Talk about SNR, role with shannon, and how it affects prediction in a reverse way here! Cite shanon!

Note on how SNR makes also the Kmeans based algo hard; i.e. the noise that is different is 
in the edges and becomes negligble.

Note on how the X indep N -> Xtilde indep N tilde only true for gaussian; for others, there 
will be dependence which the algo we propose can exploit (new one)


Briefly discuss AIC / model selection
intution about using poly reg since it's local
aprox
https://stats.stackexchange.com/questions/9171/
aic-or-p-value-which-one-to-choose-for-model-selection

Note that problem is similar to change detection
but it should be easier? -> we don't need to known
when it changes


\subsection{Ideas}

-> no free lunch theorem <-> covariate shift?




How can we tackle causality?

In the absence of noise, and the process is bijective, then it is impossible to distinghuis, 
if however, ...

Shannon answered the question: given the most simple communication system: "How reliably can 
we communicate given a certain noise level"

In some sense what we would like to answer is, given a certain noise level, how reliably can we 
predict the causal relation. 

Some points:

1. In causality we use noise, whereas in virtually all other domains
 such as communication theory the aim is combat noise.

 Interesingly yet again, the Gaussian case ends up being a difficuly case. For instance, 
 the motivation to look at the AGN additive gaussian noice channel is that the gaussian is 
 the most difficult distribution in the entropic sense; but so it is as well in the 
 bianry case setting due to:

 thm.